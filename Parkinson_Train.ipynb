{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports des bibliothèques\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Importation des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt, log\n",
    "\n",
    "# chargement des données\n",
    "parkinson_DF = pd.read_csv('train_data.txt', sep = ',', header = None)\n",
    "\n",
    "# Renommage des colonnes \n",
    "parkinson_DF.columns = ['SubjectId', \n",
    "                        'Jitter_LOCAL', 'Jitter_LOCAL_ABSOLUTE', 'Jitter_RAP','Jitter_PPQ5','Jitter_DDP', \n",
    "                        'Shimmer_LOCAL','Shimmer_LOCAL_DB','Shimmer_APQ3','Shimmer_APQ5','Shimmer_APQ11','Shimmer_DDA', \n",
    "                        'AC','NTH','HTN', \n",
    "                        'Median pitch','Mean pitch','Standard deviation','Minimum pitch','Maximum pitch',\n",
    "                        'Number of pulses','Number of periods','Mean period','Standard deviation of period',\n",
    "                        'Fraction of locally unvoiced frames','Number of voice breaks','Degree of voice breaks',\n",
    "                        'UPDRS',\n",
    "                        'Class Information']\n",
    "\n",
    "#affichage de quelques tuples\n",
    "parkinson_DF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinson_DF.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Attribute Information:</h2>\n",
    "  -- 1. SubjectId: Entier qui identifie de manière unique chaque sujet<br>   \n",
    "  \n",
    "  -- 2. Jitter_LOCAL:  mesure de variation de la fréquence fondamentale (Paramètres de fréquence)<br>\n",
    "  \n",
    "  -- 3. Jitter_LOCAL_ABSOLUTE: mesure de variation de la fréquence fondamentale absolue  (Paramètres de fréquence)<br>\n",
    "  \n",
    "  -- 4. Jitter_RAP: mesure de variation de la fréquence fondamentale (Paramètres de fréquence=Relative Amplitude Perturbation) <br>\n",
    "  \n",
    "  -- 5. Jitter_PPQ5: mesure de variation de la fréquence fondamentale (Paramètres de fréquence= five-point Period Perturbation Quotient )<br>\n",
    "  \n",
    "  -- 6. Jitter_DDP: mesure de variation de la fréquence fondamentale (Paramètres de fréquence=Average absolute difference of differences between cycles, divided by the average period)<br>\n",
    "  \n",
    "  -- 7. Shimmer_LOCAL: Mesure de variation dans l'amplitude (Amplitude parameter)<br>   \n",
    "  \n",
    "  -- 8. Shimmer_LOCAL_DB: Mesure de variation dans l'amplitude en decibels (Amplitude parameter)<br>\n",
    "  \n",
    "  -- 9. Shimmer_APQ3: Mesure de variation dans l'amplitude (Amplitude parameter= Three point Amplitude Perturbation Quotient)<br>\n",
    "  \n",
    "  -- 10. Shimmer_APQ5: Mesure de variation dans l'amplitude (Amplitude parameter=Five point Amplitude Perturbation Quotient)<br>\n",
    "  \n",
    "  -- 11. Shimmer_APQ11: Mesure de variation dans l'amplitude (Amplitude parameter=11-point Amplitude Perturbation Quotient)<br>\n",
    "  \n",
    "  -- 12. Shimmer_DDA: Mesure de variation dans l'amplitude (Amplitude parameter=Average absolute difference between consecutive differences between the amplitudes of consecutive periods )<br>\n",
    "  \n",
    "  -- 13. AC: mesure du rapport entre le bruit et les composantes tonales de la voix <br>\n",
    "  \n",
    "  -- 14. NTH: mesure du rapport entre le bruit et les composantes tonales de la voix(Noise-to-Harmonics Ratio)<br>\n",
    "  \n",
    "  -- 15. HTN: mesure du rapport entre le bruit et les composantes tonales de la voix(Harmonics-to-Noise Ratio)<br>\n",
    "  \n",
    "  -- 16. Median pitch: la mediane de pitch<br> \n",
    "  -- 17. Mean pitch: le pas (pitch) moyen<br>\n",
    "  -- 18. Standard deviation: Écart-type de pitch<br>\n",
    "  -- 19. Minimum pitch: le pitch minimale<br>\n",
    "  -- 20. Maximum pitch: le pitch maximale<br>\n",
    "  -- 21. Number of pulses: nombre d'impulsions  <br>\n",
    "  -- 22. Number of periods: nombre de periodes des impulsions<br>\n",
    "  -- 23. Mean period: periode moyenne des impulsions<br>\n",
    "  -- 24. Standard deviation of period: écart type des impulsions<br>\n",
    "  -- 25. Fraction of locally unvoiced frames: parametre de voix <br>\n",
    "  -- 26. Number of voice breaks: nombre de voix cassés (parametre de voix)<br>\n",
    "  -- 27. Degree of voice breaks: degre de voix cassé (parametre de voix)<br>\n",
    "  -- 28. UPDRS: Unified Parkinsons Disease Rating Scale score pour chaque patient<br>\n",
    "  -- 29. Class Information<br>\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Verification d'existence de valeurs nulles dans notre dataset</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.heatmap(parkinson_DF.isnull(),cbar=False,cmap='viridis',yticklabels=False)\n",
    "plt.title('Missing value in the dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'apres ce plot on vois tres bien qu'il y'a pas de valeurs manquantes dans notre dataset parkinson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Distribution of Parkinson Speak Disease</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valeur de class 1= malade sinon 0= non malade\n",
    "malade = (parkinson_DF['Class Information'] == 1)\n",
    "print(\"Nomre de gens atteintes de Parkinson Disease:\",malade.sum())\n",
    "nonMalade = (parkinson_DF['Class Information'] == 0)\n",
    "print(\"Nomre de gens non atteintes de Parkinson Disease:\",nonMalade.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Matrice de corrélation:</h4>\n",
    "Pour afficher la corrélation entre les differente données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr_matrix = parkinson_DF.corr()\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(corr_matrix, linewidths=.01, annot = True, cmap='Blues')\n",
    "#plt.savefig(\"Correlation Matrix.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix sur UPDRS\n",
    "corr_matrix['UPDRS'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la matrice de corrélation sur et de la projection sur notre label Y=UPDRS, on s'apperçoit que la variable la plus corélée et class information, mais dans notre cas on est pas entrain de faire une etude sur la classification mais c'est sur la regression, donc on vas retirer par la suite cette variable automatiquement. Donc y'a aucune corrélation de UPDRS avec les autres variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='Shimmer_APQ11',y='UPDRS',data=parkinson_DF,aspect=2,height=6)\n",
    "plt.xlabel('Shimmer_APQ11: comme variable Independante')\n",
    "plt.ylabel('UPDRS: comme variable Dependante')\n",
    "plt.title('UPDRS Vs Shimmmer_APQ11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le graphique ci-dessus, nous adaptons la ligne de régression aux variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données\n",
    "\n",
    "### Exploration uni-dimensionelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 9, ncols = 3,figsize=(10,15))\n",
    "\n",
    "i_name = 0\n",
    "            \n",
    "for i in range(9):\n",
    "    for j in range(3):\n",
    "            i_name = i_name+1\n",
    "            name_col = parkinson_DF.columns[i_name]\n",
    "            axs[i, j].hist(parkinson_DF[name_col])\n",
    "            axs[i, j].set_title(name_col)\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.savefig(\"Exploration uni dimensionelle avant transformation.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse sur Exploration uni-dimensionelle\n",
    "D'apres le plot precedent, on peut voir que y'a plusieurs colonne ou leur parition ne sont pas repartis en loi normal ce qui nous invite à gerer çà en transformant nos données utilisant SQRT et LOG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppression de quelques colonnes\n",
    "#suppression de Class Information (classification des malades) pour faire la regression\n",
    "parkinson_DF= parkinson_DF.drop('Class Information', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log\n",
    "\n",
    "#Transformation des colonnes qui sont pas reparti en loi normal\n",
    "parkinson_DF[\"LJitter_LOCAL\"] = parkinson_DF[\"Jitter_LOCAL\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"SJitter_LOCAL_ABSOLUTE\"] = parkinson_DF[\"Jitter_LOCAL_ABSOLUTE\"].map(lambda x: sqrt(x))\n",
    "parkinson_DF[\"LJitter_RAP\"] = parkinson_DF[\"Jitter_RAP\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LJitter_PPQ5\"] = parkinson_DF[\"Jitter_PPQ5\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LJitter_DDP\"] = parkinson_DF[\"Jitter_DDP\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_LOCAL\"] = parkinson_DF[\"Shimmer_LOCAL\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_APQ3\"] = parkinson_DF[\"Shimmer_APQ3\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_APQ5\"] = parkinson_DF[\"Shimmer_APQ5\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_APQ11\"] = parkinson_DF[\"Shimmer_APQ11\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_DDA\"] = parkinson_DF[\"Shimmer_DDA\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LStandard deviation\"] = parkinson_DF[\"Standard deviation\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"SNumber of pulses\"] = parkinson_DF[\"Number of pulses\"].map(lambda x: sqrt(x))\n",
    "parkinson_DF[\"SNumber of periods\"] = parkinson_DF[\"Number of periods\"].map(lambda x: sqrt(x))\n",
    "parkinson_DF[\"SStandard deviation of period\"] = parkinson_DF[\"Standard deviation of period\"].map(lambda x: sqrt(x))\n",
    "parkinson_DF[\"SNumber of voice breaks\"] = parkinson_DF[\"Number of voice breaks\"].map(lambda x: sqrt(x))\n",
    "\n",
    "\n",
    "#Suppression des anciennes colonnes\n",
    "del parkinson_DF[\"Jitter_LOCAL\"]\n",
    "del parkinson_DF[\"Jitter_LOCAL_ABSOLUTE\"]\n",
    "del parkinson_DF[\"Jitter_RAP\"]\n",
    "del parkinson_DF[\"Jitter_PPQ5\"]\n",
    "del parkinson_DF[\"Jitter_DDP\"]\n",
    "del parkinson_DF[\"Shimmer_LOCAL\"]\n",
    "del parkinson_DF[\"Shimmer_APQ3\"]\n",
    "del parkinson_DF[\"Shimmer_APQ5\"]\n",
    "del parkinson_DF[\"Shimmer_APQ11\"]\n",
    "del parkinson_DF[\"Shimmer_DDA\"]\n",
    "del parkinson_DF[\"Standard deviation\"]\n",
    "del parkinson_DF[\"Number of pulses\"]\n",
    "del parkinson_DF[\"Number of periods\"]\n",
    "del parkinson_DF[\"Standard deviation of period\"]\n",
    "del parkinson_DF[\"Number of voice breaks\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinson_DF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation uni-dimensionelle apres modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 9, ncols = 3,figsize=(10,25))\n",
    "\n",
    "i_name = 0\n",
    "            \n",
    "for i in range(9):\n",
    "    for j in range(3):\n",
    "            i_name = i_name+1\n",
    "            name_col = parkinson_DF.columns[i_name]\n",
    "            axs[i, j].hist(parkinson_DF[name_col])\n",
    "            axs[i, j].set_title(name_col)\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.savefig(\"Exploration uni dimensionelle apres transformation.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse sur Exploration uni-dimensionelle apres modification\n",
    "D'apres ce nouveau plot, on peut voir que les colonnes sont repartis en loi normalapres la transformation de nos données utilisant SQRT et LOG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exploration multi-dimensionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(parkinson_DF, alpha=0.2, figsize=(35, 35), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Création de l'ensemble d'apprentissage / validation et de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoupage de dataset pour les features et label\n",
    "X = parkinson_DF.drop('UPDRS', axis=1)\n",
    "Y = parkinson_DF['UPDRS'].copy()\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase de splittage des données\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "X_1, X_2, Y_1, Y_2 = train_test_split(X, Y, train_size = 400, test_size = 200, random_state = 42)\n",
    "\n",
    "X_av = X_1.to_numpy()\n",
    "X_t = X_2.to_numpy()\n",
    "Y_av = np.transpose([Y_1.to_numpy()])\n",
    "Y_t = np.transpose([Y_2.to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque.** Scikit-learn fonctionne avec des numpy array. Nous allons donc utiliser les indices des colonnes:\n",
    "\n",
    "0. SubjectId\n",
    "1. Shimmer_LOCAL_DB\n",
    "2. AC\n",
    "3. NTH\n",
    "4. HTN\n",
    "5. Median pitch\n",
    "6. Mean pitch\n",
    "7. Minimum pitch\n",
    "8. Maximum pitch\n",
    "9. Mean period\n",
    "11. Fraction of locally unvoiced frames\n",
    "11. Degree of voice breaks\n",
    "12. LJitter_LOCAL\n",
    "13. SJitter_LOCAL_ABSOLUTE\n",
    "14. LJitter_RAP\n",
    "15. LJitter_PPQ5\n",
    "16. LJitter_DDP\n",
    "17. LShimmer_LOCAL \n",
    "18. LShimmer_APQ3\n",
    "19. LShimmer_APQ5\n",
    "20. LShimmer_APQ11\n",
    "21. LShimmer_DDA\n",
    "22. LStandard deviation\n",
    "23. SNumber of pulses\n",
    "24. SNumber of periods\n",
    "25. SStandard deviation of period\n",
    "26. SNumber of voice breaks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage de la regression Lineaire avec ses differentes types et pénalisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Régression linéaire univariée\n",
    "\n",
    "### Relation unidimensionelle \n",
    "\n",
    "On étudie la qualité de la relation $\\hat{Y} = X$, où $\\hat{Y}$ est le vecteur de prédiction de l'UPDRS observée et $X$ le vecteur des prévisions de l'UPDRS observée calculée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2,figsize=(15,10))\n",
    "\n",
    "axs[0].plot(X_av[:,[10]], Y_av, 'o')\n",
    "axs[0].plot([0, 60], [0, 100], 'r')\n",
    "axs[0].set_xlabel('Fraction of locally unvoiced frames: $X^10 = \\hat{Y}_{av}$')\n",
    "axs[0].set_ylabel('UPDRS observée: $Y_{av}$')\n",
    "axs[0].set_title('Regression lineaire univarie')\n",
    "\n",
    "\n",
    "axs[1].plot(X_av[:,[10]], Y_av - X_av[:,[10]], 'o')\n",
    "axs[1].plot([0, 300], [0, 0], 'r')\n",
    "axs[1].set_xlabel('Fraction of locally unvoiced frames: $X^10= \\hat{Y}_{av}$')\n",
    "axs[1].set_ylabel('Résidus')\n",
    "axs[1].set_title('Graphe des Résidus')\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.savefig(\"Analyse de Fraction of locally unvoiced frames avec UPDRS.png\") \n",
    "plt.show()\n",
    "\n",
    "print(\"Coefficient de détermination :\", r2_score(Y_av, X_av[:,[10]]))\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, X_av[:,[10]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir des resultats obtenus:<br>\n",
    "- On peut voirque le R carre dans cette etude est négatif qui indique que le nombre de predictions est trop petit, et que le model est absolument pas bon, on peu meme compter à l'oeil nu le nombre de predictions.\n",
    "- Sur le deuxieme plot on peut voir que le niveau de residus est tres élevé dans trop de valeurs \"Y_av - x_10\" sont loin d'etre egale à 0, avec une difference qui arrice jusqu'à -70, ce qui fait l'augmentation des residus et aussi la baisse de R carré"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Régression linéaire ordinaire \n",
    "\n",
    "On étudie la qualité de la relation $\\hat{Y} = X\\beta + \\epsilon$, où $\\hat{Y}$ est le vecteur de prédiction de l'UPDRS observée et $X$ le vecteur des prévisions de l'UPDRS observée calculée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X_av[:,[16]], Y_av)\n",
    "Y_hat_av = ols.predict(X_av[:,[16]])\n",
    "\n",
    "print(\"Coefficient de régression : \\n\", ols.coef_)\n",
    "print()\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2, sharex = True, figsize=(15,10))\n",
    "\n",
    "axs[0].plot(X_av[:,[16]], Y_av, 'o')\n",
    "axs[0].plot(X_av[:,[16]], Y_hat_av, 'r-')\n",
    "#axs[0].set_ylim(0, 300)\n",
    "axs[0].set_xlabel('Régression Ljitter_DDP: $\\hat{Y}_{av}$')\n",
    "axs[0].set_ylabel('UPDRS observée: $Y_{av}$')\n",
    "axs[0].set_title('Regression lineaire ordinaire')\n",
    "\n",
    "axs[1].plot(Y_hat_av, Y_av - Y_hat_av, 'o')\n",
    "axs[1].plot([0, 300], [0, 0], 'r')\n",
    "axs[1].set_xlabel('Prédiction Ljitter_DDP: $\\hat{Y}_{av}$')\n",
    "axs[1].set_ylabel('Résidus')\n",
    "axs[1].set_title('Graphe des résidus')\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.savefig(\"Analyse LJitter_DDP avec UPDRS.png\") \n",
    "plt.show()\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av))\n",
    "print(\"Coefficient de détermination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir des resultats obtenus:<br>\n",
    "- On peut voir que le R carre dans cette etude est trop petit 0.07, indiquant aisni que le model est pas bon et manque de précision, on peu meme compter à l'oeil nu le nombre de predictions.\n",
    "- Sur le deuxieme plot on peut voir que le niveau de residus est tres élevé dans trop de valeurs \"Y_av - Y_hat_av\" sont loin d'etre egale à 0, avec une difference qui arrice jusqu'à 48, ce qui fait l'augmentation des residus et aussi la baisse de R carré"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Régression linéaire multivariée\n",
    "\n",
    "### Régression linéaire ordinaire\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application de la regression lineaire multivalué (𝑋𝑎𝑣,𝑌𝑎𝑣). \n",
    "from sklearn import linear_model\n",
    "\n",
    "#creation de modele de regression lineaire\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "#entrainement de model\n",
    "reg.fit(X_av, Y_av)\n",
    "\n",
    "#affichage des coeffecients de regression pour le model\n",
    "print(\"Coffecients de regression\",reg.coef_)\n",
    "print()\n",
    "\n",
    "#prediction de model sur les deux sets X_av et X_t\n",
    "Y_hat_av41 = reg.predict(X_av)\n",
    "Y_hat_t41 = reg.predict(X_t)\n",
    "\n",
    "print(\"****************Y_av **************************************\")\n",
    "print(\"Erreur quadratique (learning set) Y_av: %.2f\" % mean_squared_error(Y_av, Y_hat_av41))\n",
    "print(\"Coefficient de détermination (learning set) de Y_av: %.2f\" % r2_score(Y_av, Y_hat_av41))\n",
    "print(\"***********************************************************\")\n",
    "print()\n",
    "\n",
    "print(\"****************Y_t ***********************************\")\n",
    "print(\"Erreur quadratique (testing set) Y_t: %.2f\" % mean_squared_error(Y_t, Y_hat_t41))\n",
    "print(\"Coefficient de détermination (testing set) de Y_t: %.2f\" % r2_score(Y_t, Y_hat_t41))\n",
    "print(\"*******************************************************\")\n",
    "\n",
    "print()\n",
    "\n",
    "#affichage des coeffecients de regression \n",
    "coef = pd.Series(reg.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients pour regression lineaire multivalué\")\n",
    "#plt.savefig(\"CoeffecientsRegression Lineare multivalué.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ce plot, on aperçoit que la variable la plus impactante dans cette regression lineaire multivalué sur ce dataset est cell ci: LShimmer_DDA. <br>\n",
    "Et la valeur la moins impactante est LShimmer_APQ3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Régression linéaire pénalisée : Ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a01 = linear_model.Ridge(alpha = 0.1)\n",
    "model_r_a01 = r_a01.fit(X_av, Y_av)\n",
    "Y_hat_av_r01 = r_a01.predict(X_av)\n",
    "Y_hat_t_r01 = r_a01.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r01))\n",
    "print(\"Coefficient de détermination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r01))\n",
    "\n",
    "coef = pd.Series(model_r_a01.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Modele 1 <br>\n",
    "A partir de plot du modele 1, on aperçoit que la variable la plus impactante dans cette regression lineaire multivalué pénalisé avec Ridge d'un facteur alpha= 0.1 sur ce dataset est cell ci: \"NTH\". <br>\n",
    "Et la valeur la moins impactante est \"Snumber of pulses\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a1 = linear_model.Ridge(alpha = 1)\n",
    "model_r_a1 = r_a1.fit(X_av, Y_av)\n",
    "Y_hat_av_r1 = r_a1.predict(X_av)\n",
    "Y_hat_t_r1 = r_a1.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r1))\n",
    "print(\"Coefficient de détermination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r1))\n",
    "\n",
    "coef = pd.Series(model_r_a1.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Modele 2 <br>\n",
    "A partir de plot du modele 2, on aperçoit que la variable la plus impactante dans cette regression lineaire multivalué pénalisé avec Ridge d'un facteur alpha= 1 sur ce dataset est cell ci: \"NTH\". <br>\n",
    "Et la valeur la moins impactante est \"LShimmer_LOCAL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a10 = linear_model.Ridge(alpha = 10)\n",
    "model_r_a10 = r_a10.fit(X_av, Y_av)\n",
    "Y_hat_av_r10 = r_a10.predict(X_av)\n",
    "Y_hat_t_r10 = r_a10.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r10))\n",
    "print(\"Coefficient de détermination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r10))\n",
    "\n",
    "coef = pd.Series(model_r_a10.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Modele 3 <br>\n",
    "A partir de plot du modele 3, on aperçoit que la variable la plus impactante dans cette regression lineaire multivalué pénalisé avec Ridge d'un facteur alpha= 10 sur ce dataset est cell ci: \"LJitter_PPQ5\". <br>\n",
    "Et la valeur la moins impactante est \"Snumber of voice breaks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation globale de l'évolution des coefficients Ridge Linear Regression en fonction de alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisez globalement l'évolution des coefficients en fonction de alpha\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# #############################################################################\n",
    "# Compute paths\n",
    "\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(5, 0, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(X_t, Y_t)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "# #############################################################################\n",
    "# Display results\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "coefs = coefs.reshape(200,27)\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  #  reversed axis\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Poids\")\n",
    "plt.title(\"Ridge coefficients as a function of the L2 regularization\")\n",
    "plt.axis(\"tight\")\n",
    "#plt.savefig(\"Ridge Coeffecients Regression Lineare.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut appercevoir que le plus que alpha augmente plus les poids des coefficients tendent vers zero, et reciproquement ils devirgent de zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Linear Regression Best Alpha detection with RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mettez en place une procédure d'apprentissage rigoureuse pour trouver le paramètre alpha optimal de la régression ridge. \n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeCV = RidgeCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.5, 1,10]).fit(X_av, Y_av)\n",
    "\n",
    "print(\"Le score 'R carre' de la cross validation de Ridge Linear Regression model\", ridgeCV.score(X_av, Y_av))\n",
    "print(\"Les coeffecients du model RidgeCV Linear Regression sont:\",ridgeCV.coef_)\n",
    "print()\n",
    "#print(ridgeCV.intercept_)\n",
    "print(\"Le parametre alpha optimal pour la regression ridge est alpha = \",ridgeCV.alpha_)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "ridge_cv=RidgeCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.5, 1,10], store_cv_values=True)\n",
    "ridge_mod = ridge_cv.fit(X_av,Y_av)\n",
    "ypred = ridge_mod.predict(X_av)\n",
    "print(\"L'Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, ypred))\n",
    "\n",
    "\n",
    "x_ax = range(len(X_av))\n",
    "plt.title(\"Comparaison des resultats Y_hat_av (Prediction) avec Y(original) avec alpha = 10.0\") \n",
    "plt.scatter(x_ax,Y_av , s=10, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"Ridge Comparasion des valeurs existantes et valeurs predites.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ces resultats, dans ce dataset avec la technique de RidgeCV, on voit que le alpha le plus otimale est alpha=10.<br>\n",
    "Sur le plot suivant on peut voir la comparaison entre les valeurs initiales de UPDRS celles predites et celles non predites, ou chaque ligne rouge qui passe sur un point bleu indique une prediction et sinon cette valeur n'est pas predite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression linéaire pénalisée : Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference of lasso and ridge regression is that some of the coefficients can be zero i.e. some of the features are \n",
    "# completely neglected\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_av,Y_av)\n",
    "\n",
    "lasso01 = Lasso(alpha=0.1, max_iter=10e5)\n",
    "lasso01.fit(X_av,Y_av)\n",
    "\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n",
    "lasso001.fit(X_av,Y_av)\n",
    "\n",
    "lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\n",
    "lasso00001.fit(X_av,Y_av)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_av,Y_av)\n",
    "\n",
    "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "plt.plot(lasso01.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.1$') # alpha here is for transparency\n",
    "plt.plot(lasso001.coef_,alpha=0.7,linestyle='none',marker='o',markersize=5,color='green',label=r'Lasso; $\\alpha = 0.01$',zorder=7) # alpha here is for transparency\n",
    "\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=1)\n",
    "plt.title(\"Régression avec Lasso et dépendance de la sélection de coefficients à la valeur du alpha\")\n",
    "#plt.savefig(\"Lasso coeff regression lineaire.png\") \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso Linear Regression Best Alpha detection with LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettez en place une procédure d'apprentissage rigoureuse pour trouver le paramètre alpha optimal dans la régression lasso.\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "lassocv = LassoCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.2,0.5, 1,10]).fit(X_av, Y_av)\n",
    "print(\"Le score 'R carre' de la cross validation de Lasso Linear Regression model\", lassocv.score(X_av, Y_av))\n",
    "print(\"Les coeffecients du model LassoCV Linear Regression sont:\",lassocv.coef_)\n",
    "print()\n",
    "print(\"Le parametre alpha optimal pour la regression Lasso est alpha = \",lassocv.alpha_)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "lasso_cv=LassoCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.2,0.5, 1,10])\n",
    "lasso_mod = lasso_cv.fit(X_av,Y_av)\n",
    "ypred = lasso_mod.predict(X_av)\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, ypred))\n",
    "\n",
    "\n",
    "x_ax = range(len(X_av))\n",
    "plt.title(\"Comparaison des resultats Y_hat_av(prediction) avec Y(original) avec alpha = 0.1\") \n",
    "plt.scatter(x_ax,Y_av , s=10, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "#plt.savefig(\"Lasso Comparasion des valeurs existantes et valeurs predites.png\") \n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ces resultats, dans ce dataset avec la technique de LassoCV, on voit que le alpha le plus otimale est alpha=0.1.<br>\n",
    "Sur le plot suivant on peut voir la comparaison entre les valeurs initiales de UPDRS celles predites et celles non predites, ou chaque ligne rouge qui passe sur un point bleu indique une prediction et sinon cette valeur n'est pas predite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge vs Lasso with cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "alphas = np.logspace(-3, -1, 30)\n",
    "for Model in [Lasso, Ridge]:\n",
    "    scores = [cross_val_score(Model(alpha), X_av, Y_av, cv=3).mean()\n",
    "            for alpha in alphas]\n",
    "    plt.plot(alphas, scores, label=Model.__name__)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('cross validation score')\n",
    "plt.title('Variation des scores de RIDGE et LASSO par rapport à alpha')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"Lasso vs ridge vartion scores.png\") \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ces resultats, dans ce dataset avec la technique de cross_val_score sur les deux modeles Ridge et Lasso, on voit que plus le alpha tend vers zero, la precesion des deux modeles aussi converge vers zero, mais en montant de zero vers 0.1, la precesion du modele Lasso monte en exponentiel tant que la precision de modele Ridge monte d'une façon moins exponentionnel par rapport à Lasso. Et vers la valeur alpha = 0.1 converge ensemble, et Lasso garde toujours le dessus sur Ridge<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarques:\n",
    " On faisant ce TP, j'ai remarqué que l'inclusion de la colonne 'Class Information' a fait augmenter les performances de tout les modeles utilisées (Multivariate Linear Regression, Ridge Linear Regression et Lasso Linear Regression) au lieu d'avoir des R carre de 0.43, la performance a augmenté vers 0.62 environ.<br>\n",
    " J'ai preferé de garder le TP comme çà car je suis persuadé que (Class Information) est utile pour la classification et non pas la regression et c'est la raison à laquelle je l'ai pas prise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
