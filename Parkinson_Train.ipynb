{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports des biblioth√®ques\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Importation des donn√©es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt, log\n",
    "\n",
    "# chargement des donn√©es\n",
    "parkinson_DF = pd.read_csv('train_data.txt', sep = ',', header = None)\n",
    "\n",
    "# Renommage des colonnes \n",
    "parkinson_DF.columns = ['SubjectId', \n",
    "                        'Jitter_LOCAL', 'Jitter_LOCAL_ABSOLUTE', 'Jitter_RAP','Jitter_PPQ5','Jitter_DDP', \n",
    "                        'Shimmer_LOCAL','Shimmer_LOCAL_DB','Shimmer_APQ3','Shimmer_APQ5','Shimmer_APQ11','Shimmer_DDA', \n",
    "                        'AC','NTH','HTN', \n",
    "                        'Median pitch','Mean pitch','Standard deviation','Minimum pitch','Maximum pitch',\n",
    "                        'Number of pulses','Number of periods','Mean period','Standard deviation of period',\n",
    "                        'Fraction of locally unvoiced frames','Number of voice breaks','Degree of voice breaks',\n",
    "                        'UPDRS',\n",
    "                        'Class Information']\n",
    "\n",
    "#affichage de quelques tuples\n",
    "parkinson_DF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinson_DF.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Attribute Information:</h2>\n",
    "  -- 1. SubjectId: Entier qui identifie de mani√®re unique chaque sujet<br>   \n",
    "  \n",
    "  -- 2. Jitter_LOCAL:  mesure de variation de la fr√©quence fondamentale (Param√®tres de fr√©quence)<br>\n",
    "  \n",
    "  -- 3. Jitter_LOCAL_ABSOLUTE: mesure de variation de la fr√©quence fondamentale absolue  (Param√®tres de fr√©quence)<br>\n",
    "  \n",
    "  -- 4. Jitter_RAP: mesure de variation de la fr√©quence fondamentale (Param√®tres de fr√©quence=Relative Amplitude Perturbation) <br>\n",
    "  \n",
    "  -- 5. Jitter_PPQ5: mesure de variation de la fr√©quence fondamentale (Param√®tres de fr√©quence= five-point Period Perturbation Quotient )<br>\n",
    "  \n",
    "  -- 6. Jitter_DDP: mesure de variation de la fr√©quence fondamentale (Param√®tres de fr√©quence=Average absolute difference of differences between cycles, divided by the average period)<br>\n",
    "  \n",
    "  -- 7. Shimmer_LOCAL: Mesure de variation dans l'amplitude (Amplitude parameter)<br>   \n",
    "  \n",
    "  -- 8. Shimmer_LOCAL_DB: Mesure de variation dans l'amplitude en decibels (Amplitude parameter)<br>\n",
    "  \n",
    "  -- 9. Shimmer_APQ3: Mesure de variation dans l'amplitude (Amplitude parameter= Three point Amplitude Perturbation Quotient)<br>\n",
    "  \n",
    "  -- 10. Shimmer_APQ5: Mesure de variation dans l'amplitude (Amplitude parameter=Five point Amplitude Perturbation Quotient)<br>\n",
    "  \n",
    "  -- 11. Shimmer_APQ11: Mesure de variation dans l'amplitude (Amplitude parameter=11-point Amplitude Perturbation Quotient)<br>\n",
    "  \n",
    "  -- 12. Shimmer_DDA: Mesure de variation dans l'amplitude (Amplitude parameter=Average absolute difference between consecutive differences between the amplitudes of consecutive periods )<br>\n",
    "  \n",
    "  -- 13. AC: mesure du rapport entre le bruit et les composantes tonales de la voix <br>\n",
    "  \n",
    "  -- 14. NTH: mesure du rapport entre le bruit et les composantes tonales de la voix(Noise-to-Harmonics Ratio)<br>\n",
    "  \n",
    "  -- 15. HTN: mesure du rapport entre le bruit et les composantes tonales de la voix(Harmonics-to-Noise Ratio)<br>\n",
    "  \n",
    "  -- 16. Median pitch: la mediane de pitch<br> \n",
    "  -- 17. Mean pitch: le pas (pitch) moyen<br>\n",
    "  -- 18. Standard deviation: √âcart-type de pitch<br>\n",
    "  -- 19. Minimum pitch: le pitch minimale<br>\n",
    "  -- 20. Maximum pitch: le pitch maximale<br>\n",
    "  -- 21. Number of pulses: nombre d'impulsions  <br>\n",
    "  -- 22. Number of periods: nombre de periodes des impulsions<br>\n",
    "  -- 23. Mean period: periode moyenne des impulsions<br>\n",
    "  -- 24. Standard deviation of period: √©cart type des impulsions<br>\n",
    "  -- 25. Fraction of locally unvoiced frames: parametre de voix <br>\n",
    "  -- 26. Number of voice breaks: nombre de voix cass√©s (parametre de voix)<br>\n",
    "  -- 27. Degree of voice breaks: degre de voix cass√© (parametre de voix)<br>\n",
    "  -- 28. UPDRS: Unified Parkinsons Disease Rating Scale score pour chaque patient<br>\n",
    "  -- 29. Class Information<br>\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Verification d'existence de valeurs nulles dans notre dataset</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.heatmap(parkinson_DF.isnull(),cbar=False,cmap='viridis',yticklabels=False)\n",
    "plt.title('Missing value in the dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'apres ce plot on vois tres bien qu'il y'a pas de valeurs manquantes dans notre dataset parkinson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Distribution of Parkinson Speak Disease</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valeur de class 1= malade sinon 0= non malade\n",
    "malade = (parkinson_DF['Class Information'] == 1)\n",
    "print(\"Nomre de gens atteintes de Parkinson Disease:\",malade.sum())\n",
    "nonMalade = (parkinson_DF['Class Information'] == 0)\n",
    "print(\"Nomre de gens non atteintes de Parkinson Disease:\",nonMalade.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Matrice de corr√©lation:</h4>\n",
    "Pour afficher la corr√©lation entre les differente donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr_matrix = parkinson_DF.corr()\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(corr_matrix, linewidths=.01, annot = True, cmap='Blues')\n",
    "#plt.savefig(\"Correlation Matrix.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix sur UPDRS\n",
    "corr_matrix['UPDRS'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la matrice de corr√©lation sur et de la projection sur notre label Y=UPDRS, on s'apper√ßoit que la variable la plus cor√©l√©e et class information, mais dans notre cas on est pas entrain de faire une etude sur la classification mais c'est sur la regression, donc on vas retirer par la suite cette variable automatiquement. Donc y'a aucune corr√©lation de UPDRS avec les autres variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='Shimmer_APQ11',y='UPDRS',data=parkinson_DF,aspect=2,height=6)\n",
    "plt.xlabel('Shimmer_APQ11: comme variable Independante')\n",
    "plt.ylabel('UPDRS: comme variable Dependante')\n",
    "plt.title('UPDRS Vs Shimmmer_APQ11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le graphique ci-dessus, nous adaptons la ligne de r√©gression aux variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des donn√©es\n",
    "\n",
    "### Exploration uni-dimensionelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 9, ncols = 3,figsize=(10,15))\n",
    "\n",
    "i_name = 0\n",
    "            \n",
    "for i in range(9):\n",
    "    for j in range(3):\n",
    "            i_name = i_name+1\n",
    "            name_col = parkinson_DF.columns[i_name]\n",
    "            axs[i, j].hist(parkinson_DF[name_col])\n",
    "            axs[i, j].set_title(name_col)\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.savefig(\"Exploration uni dimensionelle avant transformation.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse sur Exploration uni-dimensionelle\n",
    "D'apres le plot precedent, on peut voir que y'a plusieurs colonne ou leur parition ne sont pas repartis en loi normal ce qui nous invite √† gerer √ß√† en transformant nos donn√©es utilisant SQRT et LOG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation du jeu de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppression de quelques colonnes\n",
    "#suppression de Class Information (classification des malades) pour faire la regression\n",
    "parkinson_DF= parkinson_DF.drop('Class Information', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log\n",
    "\n",
    "#Transformation des colonnes qui sont pas reparti en loi normal\n",
    "parkinson_DF[\"LJitter_LOCAL\"] = parkinson_DF[\"Jitter_LOCAL\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"SJitter_LOCAL_ABSOLUTE\"] = parkinson_DF[\"Jitter_LOCAL_ABSOLUTE\"].map(lambda x: sqrt(x))\n",
    "parkinson_DF[\"LJitter_RAP\"] = parkinson_DF[\"Jitter_RAP\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LJitter_PPQ5\"] = parkinson_DF[\"Jitter_PPQ5\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LJitter_DDP\"] = parkinson_DF[\"Jitter_DDP\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_LOCAL\"] = parkinson_DF[\"Shimmer_LOCAL\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_APQ3\"] = parkinson_DF[\"Shimmer_APQ3\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_APQ5\"] = parkinson_DF[\"Shimmer_APQ5\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_APQ11\"] = parkinson_DF[\"Shimmer_APQ11\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LShimmer_DDA\"] = parkinson_DF[\"Shimmer_DDA\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"LStandard deviation\"] = parkinson_DF[\"Standard deviation\"].map(lambda x: log(x))\n",
    "parkinson_DF[\"SNumber of pulses\"] = parkinson_DF[\"Number of pulses\"].map(lambda x: sqrt(x))\n",
    "parkinson_DF[\"SNumber of periods\"] = parkinson_DF[\"Number of periods\"].map(lambda x: sqrt(x))\n",
    "parkinson_DF[\"SStandard deviation of period\"] = parkinson_DF[\"Standard deviation of period\"].map(lambda x: sqrt(x))\n",
    "parkinson_DF[\"SNumber of voice breaks\"] = parkinson_DF[\"Number of voice breaks\"].map(lambda x: sqrt(x))\n",
    "\n",
    "\n",
    "#Suppression des anciennes colonnes\n",
    "del parkinson_DF[\"Jitter_LOCAL\"]\n",
    "del parkinson_DF[\"Jitter_LOCAL_ABSOLUTE\"]\n",
    "del parkinson_DF[\"Jitter_RAP\"]\n",
    "del parkinson_DF[\"Jitter_PPQ5\"]\n",
    "del parkinson_DF[\"Jitter_DDP\"]\n",
    "del parkinson_DF[\"Shimmer_LOCAL\"]\n",
    "del parkinson_DF[\"Shimmer_APQ3\"]\n",
    "del parkinson_DF[\"Shimmer_APQ5\"]\n",
    "del parkinson_DF[\"Shimmer_APQ11\"]\n",
    "del parkinson_DF[\"Shimmer_DDA\"]\n",
    "del parkinson_DF[\"Standard deviation\"]\n",
    "del parkinson_DF[\"Number of pulses\"]\n",
    "del parkinson_DF[\"Number of periods\"]\n",
    "del parkinson_DF[\"Standard deviation of period\"]\n",
    "del parkinson_DF[\"Number of voice breaks\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinson_DF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation uni-dimensionelle apres modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 9, ncols = 3,figsize=(10,25))\n",
    "\n",
    "i_name = 0\n",
    "            \n",
    "for i in range(9):\n",
    "    for j in range(3):\n",
    "            i_name = i_name+1\n",
    "            name_col = parkinson_DF.columns[i_name]\n",
    "            axs[i, j].hist(parkinson_DF[name_col])\n",
    "            axs[i, j].set_title(name_col)\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.savefig(\"Exploration uni dimensionelle apres transformation.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse sur Exploration uni-dimensionelle apres modification\n",
    "D'apres ce nouveau plot, on peut voir que les colonnes sont repartis en loi normalapres la transformation de nos donn√©es utilisant SQRT et LOG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exploration multi-dimensionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(parkinson_DF, alpha=0.2, figsize=(35, 35), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cr√©ation de l'ensemble d'apprentissage / validation et de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoupage de dataset pour les features et label\n",
    "X = parkinson_DF.drop('UPDRS', axis=1)\n",
    "Y = parkinson_DF['UPDRS'].copy()\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase de splittage des donn√©es\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "X_1, X_2, Y_1, Y_2 = train_test_split(X, Y, train_size = 400, test_size = 200, random_state = 42)\n",
    "\n",
    "X_av = X_1.to_numpy()\n",
    "X_t = X_2.to_numpy()\n",
    "Y_av = np.transpose([Y_1.to_numpy()])\n",
    "Y_t = np.transpose([Y_2.to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque.** Scikit-learn fonctionne avec des numpy array. Nous allons donc utiliser les indices des colonnes:\n",
    "\n",
    "0. SubjectId\n",
    "1. Shimmer_LOCAL_DB\n",
    "2. AC\n",
    "3. NTH\n",
    "4. HTN\n",
    "5. Median pitch\n",
    "6. Mean pitch\n",
    "7. Minimum pitch\n",
    "8. Maximum pitch\n",
    "9. Mean period\n",
    "11. Fraction of locally unvoiced frames\n",
    "11. Degree of voice breaks\n",
    "12. LJitter_LOCAL\n",
    "13. SJitter_LOCAL_ABSOLUTE\n",
    "14. LJitter_RAP\n",
    "15. LJitter_PPQ5\n",
    "16. LJitter_DDP\n",
    "17. LShimmer_LOCAL \n",
    "18. LShimmer_APQ3\n",
    "19. LShimmer_APQ5\n",
    "20. LShimmer_APQ11\n",
    "21. LShimmer_DDA\n",
    "22. LStandard deviation\n",
    "23. SNumber of pulses\n",
    "24. SNumber of periods\n",
    "25. SStandard deviation of period\n",
    "26. SNumber of voice breaks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage de la regression Lineaire avec ses differentes types et p√©nalisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  R√©gression lin√©aire univari√©e\n",
    "\n",
    "### Relation unidimensionelle \n",
    "\n",
    "On √©tudie la qualit√© de la relation $\\hat{Y} = X$, o√π $\\hat{Y}$ est le vecteur de pr√©diction de l'UPDRS observ√©e et $X$ le vecteur des pr√©visions de l'UPDRS observ√©e calcul√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2,figsize=(15,10))\n",
    "\n",
    "axs[0].plot(X_av[:,[10]], Y_av, 'o')\n",
    "axs[0].plot([0, 60], [0, 100], 'r')\n",
    "axs[0].set_xlabel('Fraction of locally unvoiced frames: $X^10 = \\hat{Y}_{av}$')\n",
    "axs[0].set_ylabel('UPDRS observ√©e: $Y_{av}$')\n",
    "axs[0].set_title('Regression lineaire univarie')\n",
    "\n",
    "\n",
    "axs[1].plot(X_av[:,[10]], Y_av - X_av[:,[10]], 'o')\n",
    "axs[1].plot([0, 300], [0, 0], 'r')\n",
    "axs[1].set_xlabel('Fraction of locally unvoiced frames: $X^10= \\hat{Y}_{av}$')\n",
    "axs[1].set_ylabel('R√©sidus')\n",
    "axs[1].set_title('Graphe des R√©sidus')\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.savefig(\"Analyse de Fraction of locally unvoiced frames avec UPDRS.png\") \n",
    "plt.show()\n",
    "\n",
    "print(\"Coefficient de d√©termination :\", r2_score(Y_av, X_av[:,[10]]))\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, X_av[:,[10]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir des resultats obtenus:<br>\n",
    "- On peut voirque le R carre dans cette etude est n√©gatif qui indique que le nombre de predictions est trop petit, et que le model est absolument pas bon, on peu meme compter √† l'oeil nu le nombre de predictions.\n",
    "- Sur le deuxieme plot on peut voir que le niveau de residus est tres √©lev√© dans trop de valeurs \"Y_av - x_10\" sont loin d'etre egale √† 0, avec une difference qui arrice jusqu'√† -70, ce qui fait l'augmentation des residus et aussi la baisse de R carr√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### R√©gression lin√©aire ordinaire \n",
    "\n",
    "On √©tudie la qualit√© de la relation $\\hat{Y} = X\\beta + \\epsilon$, o√π $\\hat{Y}$ est le vecteur de pr√©diction de l'UPDRS observ√©e et $X$ le vecteur des pr√©visions de l'UPDRS observ√©e calcul√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X_av[:,[16]], Y_av)\n",
    "Y_hat_av = ols.predict(X_av[:,[16]])\n",
    "\n",
    "print(\"Coefficient de r√©gression : \\n\", ols.coef_)\n",
    "print()\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2, sharex = True, figsize=(15,10))\n",
    "\n",
    "axs[0].plot(X_av[:,[16]], Y_av, 'o')\n",
    "axs[0].plot(X_av[:,[16]], Y_hat_av, 'r-')\n",
    "#axs[0].set_ylim(0, 300)\n",
    "axs[0].set_xlabel('R√©gression Ljitter_DDP: $\\hat{Y}_{av}$')\n",
    "axs[0].set_ylabel('UPDRS observ√©e: $Y_{av}$')\n",
    "axs[0].set_title('Regression lineaire ordinaire')\n",
    "\n",
    "axs[1].plot(Y_hat_av, Y_av - Y_hat_av, 'o')\n",
    "axs[1].plot([0, 300], [0, 0], 'r')\n",
    "axs[1].set_xlabel('Pr√©diction Ljitter_DDP: $\\hat{Y}_{av}$')\n",
    "axs[1].set_ylabel('R√©sidus')\n",
    "axs[1].set_title('Graphe des r√©sidus')\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.savefig(\"Analyse LJitter_DDP avec UPDRS.png\") \n",
    "plt.show()\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av))\n",
    "print(\"Coefficient de d√©termination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir des resultats obtenus:<br>\n",
    "- On peut voir que le R carre dans cette etude est trop petit 0.07, indiquant aisni que le model est pas bon et manque de pr√©cision, on peu meme compter √† l'oeil nu le nombre de predictions.\n",
    "- Sur le deuxieme plot on peut voir que le niveau de residus est tres √©lev√© dans trop de valeurs \"Y_av - Y_hat_av\" sont loin d'etre egale √† 0, avec une difference qui arrice jusqu'√† 48, ce qui fait l'augmentation des residus et aussi la baisse de R carr√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  R√©gression lin√©aire multivari√©e\n",
    "\n",
    "### R√©gression lin√©aire ordinaire\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application de la regression lineaire multivalu√© (ùëãùëéùë£,ùëåùëéùë£). \n",
    "from sklearn import linear_model\n",
    "\n",
    "#creation de modele de regression lineaire\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "#entrainement de model\n",
    "reg.fit(X_av, Y_av)\n",
    "\n",
    "#affichage des coeffecients de regression pour le model\n",
    "print(\"Coffecients de regression\",reg.coef_)\n",
    "print()\n",
    "\n",
    "#prediction de model sur les deux sets X_av et X_t\n",
    "Y_hat_av41 = reg.predict(X_av)\n",
    "Y_hat_t41 = reg.predict(X_t)\n",
    "\n",
    "print(\"****************Y_av **************************************\")\n",
    "print(\"Erreur quadratique (learning set) Y_av: %.2f\" % mean_squared_error(Y_av, Y_hat_av41))\n",
    "print(\"Coefficient de d√©termination (learning set) de Y_av: %.2f\" % r2_score(Y_av, Y_hat_av41))\n",
    "print(\"***********************************************************\")\n",
    "print()\n",
    "\n",
    "print(\"****************Y_t ***********************************\")\n",
    "print(\"Erreur quadratique (testing set) Y_t: %.2f\" % mean_squared_error(Y_t, Y_hat_t41))\n",
    "print(\"Coefficient de d√©termination (testing set) de Y_t: %.2f\" % r2_score(Y_t, Y_hat_t41))\n",
    "print(\"*******************************************************\")\n",
    "\n",
    "print()\n",
    "\n",
    "#affichage des coeffecients de regression \n",
    "coef = pd.Series(reg.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients pour regression lineaire multivalu√©\")\n",
    "#plt.savefig(\"CoeffecientsRegression Lineare multivalu√©.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ce plot, on aper√ßoit que la variable la plus impactante dans cette regression lineaire multivalu√© sur ce dataset est cell ci: LShimmer_DDA. <br>\n",
    "Et la valeur la moins impactante est LShimmer_APQ3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  R√©gression lin√©aire p√©nalis√©e : Ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mod√®le 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a01 = linear_model.Ridge(alpha = 0.1)\n",
    "model_r_a01 = r_a01.fit(X_av, Y_av)\n",
    "Y_hat_av_r01 = r_a01.predict(X_av)\n",
    "Y_hat_t_r01 = r_a01.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r01))\n",
    "print(\"Coefficient de d√©termination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r01))\n",
    "\n",
    "coef = pd.Series(model_r_a01.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Modele 1 <br>\n",
    "A partir de plot du modele 1, on aper√ßoit que la variable la plus impactante dans cette regression lineaire multivalu√© p√©nalis√© avec Ridge d'un facteur alpha= 0.1 sur ce dataset est cell ci: \"NTH\". <br>\n",
    "Et la valeur la moins impactante est \"Snumber of pulses\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mod√®le 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a1 = linear_model.Ridge(alpha = 1)\n",
    "model_r_a1 = r_a1.fit(X_av, Y_av)\n",
    "Y_hat_av_r1 = r_a1.predict(X_av)\n",
    "Y_hat_t_r1 = r_a1.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r1))\n",
    "print(\"Coefficient de d√©termination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r1))\n",
    "\n",
    "coef = pd.Series(model_r_a1.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Modele 2 <br>\n",
    "A partir de plot du modele 2, on aper√ßoit que la variable la plus impactante dans cette regression lineaire multivalu√© p√©nalis√© avec Ridge d'un facteur alpha= 1 sur ce dataset est cell ci: \"NTH\". <br>\n",
    "Et la valeur la moins impactante est \"LShimmer_LOCAL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mod√®le 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a10 = linear_model.Ridge(alpha = 10)\n",
    "model_r_a10 = r_a10.fit(X_av, Y_av)\n",
    "Y_hat_av_r10 = r_a10.predict(X_av)\n",
    "Y_hat_t_r10 = r_a10.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r10))\n",
    "print(\"Coefficient de d√©termination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r10))\n",
    "\n",
    "coef = pd.Series(model_r_a10.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Modele 3 <br>\n",
    "A partir de plot du modele 3, on aper√ßoit que la variable la plus impactante dans cette regression lineaire multivalu√© p√©nalis√© avec Ridge d'un facteur alpha= 10 sur ce dataset est cell ci: \"LJitter_PPQ5\". <br>\n",
    "Et la valeur la moins impactante est \"Snumber of voice breaks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation globale de l'√©volution des coefficients Ridge Linear Regression en fonction de alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisez globalement l'√©volution des coefficients en fonction de alpha\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# #############################################################################\n",
    "# Compute paths\n",
    "\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(5, 0, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(X_t, Y_t)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "# #############################################################################\n",
    "# Display results\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "coefs = coefs.reshape(200,27)\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  #  reversed axis\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Poids\")\n",
    "plt.title(\"Ridge coefficients as a function of the L2 regularization\")\n",
    "plt.axis(\"tight\")\n",
    "#plt.savefig(\"Ridge Coeffecients Regression Lineare.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut appercevoir que le plus que alpha augmente plus les poids des coefficients tendent vers zero, et reciproquement ils devirgent de zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Linear Regression Best Alpha detection with RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mettez en place une proc√©dure d'apprentissage rigoureuse pour trouver le param√®tre alpha optimal de la r√©gression ridge. \n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeCV = RidgeCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.5, 1,10]).fit(X_av, Y_av)\n",
    "\n",
    "print(\"Le score 'R carre' de la cross validation de Ridge Linear Regression model\", ridgeCV.score(X_av, Y_av))\n",
    "print(\"Les coeffecients du model RidgeCV Linear Regression sont:\",ridgeCV.coef_)\n",
    "print()\n",
    "#print(ridgeCV.intercept_)\n",
    "print(\"Le parametre alpha optimal pour la regression ridge est alpha = \",ridgeCV.alpha_)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "ridge_cv=RidgeCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.5, 1,10], store_cv_values=True)\n",
    "ridge_mod = ridge_cv.fit(X_av,Y_av)\n",
    "ypred = ridge_mod.predict(X_av)\n",
    "print(\"L'Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, ypred))\n",
    "\n",
    "\n",
    "x_ax = range(len(X_av))\n",
    "plt.title(\"Comparaison des resultats Y_hat_av (Prediction) avec Y(original) avec alpha = 10.0\") \n",
    "plt.scatter(x_ax,Y_av , s=10, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"Ridge Comparasion des valeurs existantes et valeurs predites.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ces resultats, dans ce dataset avec la technique de RidgeCV, on voit que le alpha le plus otimale est alpha=10.<br>\n",
    "Sur le plot suivant on peut voir la comparaison entre les valeurs initiales de UPDRS celles predites et celles non predites, ou chaque ligne rouge qui passe sur un point bleu indique une prediction et sinon cette valeur n'est pas predite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©gression lin√©aire p√©nalis√©e : Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference of lasso and ridge regression is that some of the coefficients can be zero i.e. some of the features are \n",
    "# completely neglected\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_av,Y_av)\n",
    "\n",
    "lasso01 = Lasso(alpha=0.1, max_iter=10e5)\n",
    "lasso01.fit(X_av,Y_av)\n",
    "\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n",
    "lasso001.fit(X_av,Y_av)\n",
    "\n",
    "lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\n",
    "lasso00001.fit(X_av,Y_av)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_av,Y_av)\n",
    "\n",
    "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "plt.plot(lasso01.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.1$') # alpha here is for transparency\n",
    "plt.plot(lasso001.coef_,alpha=0.7,linestyle='none',marker='o',markersize=5,color='green',label=r'Lasso; $\\alpha = 0.01$',zorder=7) # alpha here is for transparency\n",
    "\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=1)\n",
    "plt.title(\"R√©gression avec Lasso et d√©pendance de la s√©lection de coefficients √† la valeur du alpha\")\n",
    "#plt.savefig(\"Lasso coeff regression lineaire.png\") \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso Linear Regression Best Alpha detection with LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettez en place une proc√©dure d'apprentissage rigoureuse pour trouver le param√®tre alpha optimal dans la r√©gression lasso.\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "lassocv = LassoCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.2,0.5, 1,10]).fit(X_av, Y_av)\n",
    "print(\"Le score 'R carre' de la cross validation de Lasso Linear Regression model\", lassocv.score(X_av, Y_av))\n",
    "print(\"Les coeffecients du model LassoCV Linear Regression sont:\",lassocv.coef_)\n",
    "print()\n",
    "print(\"Le parametre alpha optimal pour la regression Lasso est alpha = \",lassocv.alpha_)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "lasso_cv=LassoCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.2,0.5, 1,10])\n",
    "lasso_mod = lasso_cv.fit(X_av,Y_av)\n",
    "ypred = lasso_mod.predict(X_av)\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, ypred))\n",
    "\n",
    "\n",
    "x_ax = range(len(X_av))\n",
    "plt.title(\"Comparaison des resultats Y_hat_av(prediction) avec Y(original) avec alpha = 0.1\") \n",
    "plt.scatter(x_ax,Y_av , s=10, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "#plt.savefig(\"Lasso Comparasion des valeurs existantes et valeurs predites.png\") \n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ces resultats, dans ce dataset avec la technique de LassoCV, on voit que le alpha le plus otimale est alpha=0.1.<br>\n",
    "Sur le plot suivant on peut voir la comparaison entre les valeurs initiales de UPDRS celles predites et celles non predites, ou chaque ligne rouge qui passe sur un point bleu indique une prediction et sinon cette valeur n'est pas predite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge vs Lasso with cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "alphas = np.logspace(-3, -1, 30)\n",
    "for Model in [Lasso, Ridge]:\n",
    "    scores = [cross_val_score(Model(alpha), X_av, Y_av, cv=3).mean()\n",
    "            for alpha in alphas]\n",
    "    plt.plot(alphas, scores, label=Model.__name__)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('cross validation score')\n",
    "plt.title('Variation des scores de RIDGE et LASSO par rapport √† alpha')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"Lasso vs ridge vartion scores.png\") \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ces resultats, dans ce dataset avec la technique de cross_val_score sur les deux modeles Ridge et Lasso, on voit que plus le alpha tend vers zero, la precesion des deux modeles aussi converge vers zero, mais en montant de zero vers 0.1, la precesion du modele Lasso monte en exponentiel tant que la precision de modele Ridge monte d'une fa√ßon moins exponentionnel par rapport √† Lasso. Et vers la valeur alpha = 0.1 converge ensemble, et Lasso garde toujours le dessus sur Ridge<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarques:\n",
    " On faisant ce TP, j'ai remarqu√© que l'inclusion de la colonne 'Class Information' a fait augmenter les performances de tout les modeles utilis√©es (Multivariate Linear Regression, Ridge Linear Regression et Lasso Linear Regression) au lieu d'avoir des R carre de 0.43, la performance a augment√© vers 0.62 environ.<br>\n",
    " J'ai prefer√© de garder le TP comme √ß√† car je suis persuad√© que (Class Information) est utile pour la classification et non pas la regression et c'est la raison √† laquelle je l'ai pas prise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
