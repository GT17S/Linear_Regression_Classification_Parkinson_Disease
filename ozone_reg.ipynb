{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP de r√©gression lin√©aire\n",
    "\n",
    "**Cr√©dits.** Ce TP est largement inspir√© du sc√©nario disponible sur Wikistat.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### Objectif\n",
    "\n",
    "L'objectif est d'am√©liorer la pr√©vision d√©terministe (MOCAGE), calcul√©e par les services de M√©t√©o France, de la concentration d'ozone dans certaines stations de pr√©l√®vement. Il s'agit d'un probl√®me dit d'adaptation statistique ou post-traitement d'une pr√©vision locale de mod√®les √† trop grande √©chelle en s'aidant d'autre variables √©galement g√©r√©es par M√©t√©oFrance, mais √† plus petite √©chelle (temp√©rature, force du vent, etc.).\n",
    "\n",
    "### Description des donn√©es \n",
    "Les donn√©es ont √©t√© extraites et mises en forme par le service concern√© de M√©t√©o France. Elles sont d√©crites par les variables suivantes:\n",
    "- JOUR : Type de jour (f√©ri√© (1) ou pas (0) ;\n",
    "- O3obs : Concentration d'ozone effectivement observ√©e le lendemain √† 17h locales correspondant souvent au maximum de pollution observ√©e ;\n",
    "- MOCAGE : Pr√©vision de cette pollution obtenue par un mod√®le d√©terministe de m√©canique des fluides (√©quation de Navier et Stockes);\n",
    "- TEMPE : Temp√©rature pr√©vue par M√©t√©oFrance pour le lendemain 17h ;\n",
    "- RMH2O : Rapport d'humidit√© ;\n",
    "- NO2 : Concentration en dioxyde d'azote ;\n",
    "- NO : Concentration en monoxyde d'azote ;\n",
    "- STATION : Lieu de l'observation (Aix-en-Provence, Rambouillet, Munchhausen, Cadarache et Plan de Cuques) ;\n",
    "- VentMOD : Force du vent ;\n",
    "- VentANG : Orientation du vent.\n",
    "\n",
    "Ce sont des donn√©es bien cod√©es et de petite taille. Elles pr√©sentent avant tout un caract√®re p√©dagogique.\n",
    "\n",
    "\n",
    "## 2. Prise en main des donn√©es\n",
    "\n",
    "Afin de charger et d'√©tudier les donn√©es, nous allons utiliser la librairie pandas pour b√©n√©ficier de la classe DataFrame.\n",
    "\n",
    "\n",
    "### 2.1. Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt, log\n",
    "\n",
    "# chargement des donn√©es\n",
    "Ozone_DF = pd.read_csv('depSeuil.csv', sep = ',', header = 0)\n",
    "Ozone_DF.head()\n",
    "\n",
    "\n",
    "# typage des donn√©es\n",
    "Ozone_DF[\"STATION\"] = pd.Categorical(Ozone_DF[\"STATION\"], ordered = False)\n",
    "Ozone_DF[\"JOUR\"] = pd.Categorical(Ozone_DF[\"JOUR\"], ordered = False)\n",
    "Ozone_DF[\"O3obs\"] = pd.DataFrame(Ozone_DF[\"O3obs\"], dtype = float)\n",
    "\n",
    "Ozone_DF.dtypes\n",
    "Ozone_DF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ozone_DF.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploration des donn√©es\n",
    "\n",
    "#### Exploration uni-dimensionelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 3)\n",
    "\n",
    "i_name = 0\n",
    "            \n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "            i_name = i_name+1\n",
    "            name_col = Ozone_DF.columns[i_name]\n",
    "            axs[i, j].hist(Ozone_DF[name_col])\n",
    "            axs[i, j].set_title(name_col)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation du jeu de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log\n",
    "\n",
    "Ozone_DF[\"SRMH2O\"] = Ozone_DF[\"RMH2O\"].map(lambda x: sqrt(x))\n",
    "Ozone_DF[\"LNO2\"] = Ozone_DF[\"NO2\"].map(lambda x: log(x))\n",
    "Ozone_DF[\"LNO\"] = Ozone_DF[\"NO\"].map(lambda x: log(x))\n",
    "del Ozone_DF[\"RMH2O\"]\n",
    "del Ozone_DF[\"NO2\"]\n",
    "del Ozone_DF[\"NO\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ozone_DF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H6>quelles sont les transformations appliqu√©es ci-dessus, et dans quel but</H6><br>\n",
    "On fait appel √† deux fonctions de la librairie Math et on les appliques sur certains colonnes de donn√©es de DataFrame Ozone_DF comme suit:<br>\n",
    "1- On fait une racine caree sur toutes les valeurs de la colonne \"RMH2O\" puis par la suite on affecte le resultat dans une autre nouvelle colonne cr√©e qui s'appelle \"SRMH2O\".  <br>\n",
    "2- On fait un logarithme (default base= 'e') sur toutes les valeurs de la colonne \"N2O\" puis par la suite on affecte le resultat dans une autre nouvelle colonne cr√©e qui s'appelle \"LN2O\".<br>\n",
    "3- On fait un logarithme (default base= 'e') sur toutes les valeurs de la colonne \"NO\" puis par la suite on affecte le resultat dans une autre nouvelle colonne cr√©e qui s'appelle \"LNO\".<br>\n",
    "4- On supprime les anciennes colonnes \"RMH2O\", \"N2O\" et \"NO\" du dataframe Ozone_DF.<br>\n",
    "<br>En cas de probl√®mes, le rem√®de consiste souvent √† rechercher des transformations des variables permettant de rendre les distributions sym√©triques, de ‚Äúbanaliser‚Äù les points atypiques et de rendre lin√©aire la relation. Et c'est le but de ces deux transformations est celui de transformer la relation en relation lin√©aire. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Quelles sont les transformations appliqu√©es ci-dessus, et dans quel but ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ozone_Dum = pd.get_dummies(Ozone_DF[[\"JOUR\", \"STATION\"]])\n",
    "del Ozone_Dum[\"JOUR_0\"]\n",
    "\n",
    "Ozone_Quant = Ozone_DF[[\"MOCAGE\", \"TEMPE\", \"VentMOD\", \"VentANG\", \"SRMH2O\", \"LNO2\", \"LNO\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ozone_Dum.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ozone_Quant.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H6>quelles sont les transformations appliqu√©es ci-dessus, et dans quel but (2)</H6><br>\n",
    "On fait appel une fonction du la librairie pandas qui est \"get_dummies\" on l'applique sur certains colonnes de donn√©es de DataFrame Ozone_DF comme suit:<br>\n",
    "1- On fait appel a get_dummies pour convertir les variables cat√©gorielles en variables fictives(indicateurs), et notamment sur la colonne \"STATION\" de Ozone_DF et aussi la colonne \"JOUR\" de Ozone_DF, par la suite on les stocke dans un Dataframe Ozone_Dum. <br>\n",
    "2- Par la suite on supprime la colonne JOUR_0 du dataframe Ozone_Dum. <br>\n",
    "3- Par la suite on cree un dataframe qui Ozone_Quan qui vas contenir tous les variables/colonnes quantitatives du Ozone_DF (\"MOCAGE\", \"TEMPE\", \"VentMOD\", \"VentANG\", \"SRMH2O\", \"LNO2\", \"LNO\").<br>\n",
    "<br>Le but d'application de cet ensemble de transformations est de s√©parer les donn√©es categorielles (qu'on vas faire des etudes uniquement sur les jours feriers car on a supprimer la colonne JOUR_0 qui represente des jours normales) des donn√©es quantitatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Quelles sont les transformations appliqu√©es ci-dessus, et dans quel but ?\n",
    "\n",
    "#### Exploration multi-dimensionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(Ozone_DF[[\"O3obs\", \"MOCAGE\", \"TEMPE\", \"VentMOD\", \"VentANG\", \"SRMH2O\", \"LNO2\", \"LNO\"]], alpha=0.2, figsize=(15, 15), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cr√©ation de l'ensemble d'apprentissage / validation et de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([Ozone_Dum, Ozone_Quant], axis = 1)\n",
    "Y = Ozone_DF[\"O3obs\"]\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "X_1, X_2, Y_1, Y_2 = train_test_split(X, Y, train_size = 400, test_size = 200, random_state = 42)\n",
    "\n",
    "X_av = X_1.to_numpy()\n",
    "X_t = X_2.to_numpy()\n",
    "Y_av = np.transpose([Y_1.to_numpy()])\n",
    "Y_t = np.transpose([Y_2.to_numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque.** Scikit-learn fonctionne avec des numpy array. Nous allons donc utiliser les indices des colonnes:\n",
    "\n",
    "0. JOUR\n",
    "1. STATION_Aix \n",
    "2. STATION_Als \n",
    "3. STATION_Cad \n",
    "4. STATION_Pla\n",
    "5. STATION_Ram\n",
    "6. MOCAGE\n",
    "7. TEMPE\n",
    "8. VentMOD\n",
    "9. VentANG\n",
    "10. SRMH2O\n",
    "11. LNO2\n",
    "12. LNO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. R√©gression lin√©aire univari√©e\n",
    "\n",
    "### 3.1. Relation unidimensionelle \n",
    "\n",
    "On √©tudie la qualit√© de la relation $\\hat{Y} = X$, o√π $\\hat{Y}$ est le vecteur de pr√©diction de l'O3 observ√©e et $X$ le vecteur des pr√©visions de l'O3 observ√©e calcul√©e par M√©t√©o France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2)\n",
    "\n",
    "axs[0].plot(X_av[:,[6]], Y_av, 'o')\n",
    "axs[0].plot([0, 300], [0, 300], 'r')\n",
    "axs[0].set_xlabel('Mocage: $X^6 = \\hat{Y}_{av}$')\n",
    "axs[0].set_ylabel('O3 observ√©e: $Y_{av}$')\n",
    "\n",
    "axs[1].plot(X_av[:,[6]], Y_av - X_av[:,[6]], 'o')\n",
    "axs[1].plot([0, 300], [0, 0], 'r')\n",
    "axs[1].set_xlabel('Mocage: $X^6= \\hat{Y}_{av}$')\n",
    "axs[1].set_ylabel('R√©sidus')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Coefficient de d√©termination :\", r2_score(Y_av, X_av[:,[6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Commentaires:</h6><br>\n",
    "On peut dire que le coeffecient de determination R carre est tres petit avec une valeur de 0.175 approximativement, ce qui rend ce modele de Y_hat_av = X^6 (mocage) pas de tout efficace, et on vois passer aussi la droite de regression par l'origine deplan mais par sur toutes les valeurs de Y_av ou elle passe pas sur, et avec un niveau de residus tres √©lev√© dans le deuxieme graphe(trop de valeurs \"Y_hat_av - x_mocage\" sont loin d'etre egale √† 0, avec une difference qui arrice jusqu'√† 150, ce qui fait l'augmentation des residus et aussi la baisse de R carr√©)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Commentez les r√©sultats.\n",
    "\n",
    "### 3.2. R√©gression lin√©aire ordinaire \n",
    "\n",
    "On √©tudie la qualit√© de la relation $\\hat{Y} = X\\beta + \\epsilon$, o√π $\\hat{Y}$ est le vecteur de pr√©diction de l'O3 observ√©e et $X$ le vecteur des pr√©visions de l'O3 observ√©e calcul√©e par M√©t√©o France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X_av[:,[6]], Y_av)\n",
    "Y_hat_av = ols.predict(X_av[:,[6]])\n",
    "\n",
    "print(\"Coefficient de r√©gression : \\n\", ols.coef_)\n",
    "print()\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2, sharex = True)\n",
    "\n",
    "axs[0].plot(X_av[:,[6]], Y_av, 'o')\n",
    "axs[0].plot(X_av[:,[6]], Y_hat_av, 'r-')\n",
    "#axs[0].set_ylim(0, 300)\n",
    "axs[0].set_xlabel('R√©gression Mocage: $\\hat{Y}_{av}$')\n",
    "axs[0].set_ylabel('O3 observ√©e: $Y_{av}$')\n",
    "\n",
    "axs[1].plot(Y_hat_av, Y_av - Y_hat_av, 'o')\n",
    "axs[1].plot([0, 300], [0, 0], 'r')\n",
    "axs[1].set_xlabel('Pr√©diction Mocage: $\\hat{Y}_{av}$')\n",
    "axs[1].set_ylabel('R√©sidus')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av))\n",
    "print(\"Coefficient de d√©termination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Un seul coeff:</h6><br>\n",
    "1- On a un seul coeffecient parceque lors du Fit de modele ols (modele de regression lineare) on utlise simplement une seule variable indepandente qui est X6_Mocage du dataframe X_av.\n",
    "<br><h6>Commentaires:</h6><br>\n",
    "On peut dire que le coeffecient de determination R carre est petit avec une valeur de 0.35 approximativement, ce qui rend ce modele de Y_hat_av = beta * X^6 (mocage) pas de tout efficace(tel qu'il est le cas pour Y_hat_av=x6_mocage), et on vois passer la droite de regression par certains  points (valeurs <==> que y'a certaines predictions juste) de Y_av mais pas tous, et on a moyenne de la diff√©rence au carr√© des valeurs r√©elles par rapport aux valeurs pr√©vues (MSE) √©gale √† 1125,92 qui comme meme trop grand. Et avec un niveau de residus tres √©lev√© dans le deuxieme graphe(trop de valeurs residus \"Y_av - Y_hat_av\" sont loin d'etre egale √† 0, ce qui fait l'augmentation des residus et aussi la baisse de R carr√© )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions.**\n",
    "\n",
    "1. Pourquoi n'y a-t-il qu'un seul coefficient ?\n",
    "2. Commentez les r√©sultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. R√©gression lin√©aire multivari√©e\n",
    "\n",
    "### 4.1. R√©gression lin√©aire ordinaire\n",
    "\n",
    "**Exercice.** \n",
    "1. Appliquez une r√©gression lin√©aire sur l'ensemble d'apprentissage $(X_{av}, Y_{av})$ et √©valuez la qualit√© des r√©sultats en comparant ceux obtenus sur $(X_{av}, Y_{av})$ et sur $(X_{t}, Y_{t})$. Commentez.\n",
    "2. Observez les coefficients de r√©gression et commentez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application de la regression lineaire multivalu√© (ùëãùëéùë£,ùëåùëéùë£). \n",
    "from sklearn import linear_model\n",
    "\n",
    "#creation de modele de regression lineaire\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "#entrainement de model\n",
    "reg.fit(X_av, Y_av)\n",
    "\n",
    "#affichage des coeffecients de regression pour le model\n",
    "print(\"Coffecients de regression\",reg.coef_)\n",
    "print()\n",
    "\n",
    "#prediction de model sur les deux sets X_av et X_t\n",
    "Y_hat_av41 = reg.predict(X_av)\n",
    "Y_hat_t41 = reg.predict(X_t)\n",
    "\n",
    "print(\"****************Y_av ****************************\")\n",
    "print(\"Erreur quadratique (learning set) Y_av: %.2f\" % mean_squared_error(Y_av, Y_hat_av41))\n",
    "print(\"Coefficient de d√©termination (learning set) de Y_av: %.2f\" % r2_score(Y_av, Y_hat_av41))\n",
    "print(\"*************************************************\")\n",
    "print()\n",
    "\n",
    "print(\"****************Y_t *****************************\")\n",
    "print(\"Erreur quadratique (testing set) Y_t: %.2f\" % mean_squared_error(Y_t, Y_hat_t41))\n",
    "print(\"Coefficient de d√©termination (testing set) de Y_t: %.2f\" % r2_score(Y_t, Y_hat_t41))\n",
    "print(\"**************************************************\")\n",
    "\n",
    "print()\n",
    "\n",
    "#affichage des coeffecients de regression pour question 2\n",
    "coef = pd.Series(reg.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients pour regression lineaire multivalu√©\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Question 1:</h6><br> \n",
    "A partir des resultats obtenu, le coeffecient de determination (R carre) et la moyenne de la diff√©rence au carr√© des valeurs r√©elles par rapport aux valeurs pr√©vues (MSE), on peut voir clairement que la qualit√© de couple (ùëãùëéùë£,ùëåùëéùë£) est mielleur de l'autre couple (ùëãùë°,ùëåùë°), ou on peu voir que le R carre de Y_hat_av41 du couple (ùëãav,ùëåav) est superieur √† celui de (ùëãùë°,ùëåùë°), 0.57 > 0.42 qui implique que l'accuracy de model est mielleur sur le premier couple que au deuxieme. Et aussi on vois que le MSE de (ùëãùë°,ùëåùë°) est superieur √† celui de MSE (ùëãav,ùëåav), 959.51 > 742.01 qui veut dire que le nombre de r√©sidus est sup√©rieur sur le couple (ùëãùë°,ùëåùë°) qu'au couple (ùëãav,ùëåav), ainsi √©xpliquant la qualit√© surpassante de modele de regression sur le couple (ùëãav,ùëåav) √† celui de (ùëãùë°,ùëåùë°).\n",
    "\n",
    "\n",
    "<br><h6>Question 2:</h6><br>\n",
    "On peut voir sur le plot pr√©cedent (Coefficients pour regression lineaire multivalu√©) que les variables les plus impactantes sur le modeles sont:<br>\n",
    "on a au premier lieu, SRMHO2 est la variable indepandante la plus impactante apres on a LNO et ainsi de suite jusqu'au moins important qui est LNO2.<br>\n",
    "Et on peut aussi voir que ce modele de regression il a pris tout les variables indepandantes, car elles sont toutes represent√©s sur le plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. R√©gression lin√©aire p√©nalis√©e : Ridge\n",
    "\n",
    "**Question**.  Rappelez le mod√®le de la r√©gression ridge et le comportement attendu.\n",
    "\n",
    "Nous allons observer sur l'ensemble d'apprentissage, pour 3 valeurs de alpha ($\\lambda$ dans le cours), le comportement de la regression ridge sur les coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Question</h6>RIDGE Regression model<br>\n",
    "La r√©gression ridge consiste √† minimiser le crit√®re des moindres carr√©s p√©nalis√© par la norme 2 des coefficients.\n",
    "<br>\n",
    "Parmis les comportements attendu du Ridge on a:<br>\n",
    "Si Œª (alpha de Ridge) = 0, on retrouve le biais et la variance de l‚Äôestimateur des Moindres Carres Ordinaire (y'aura pas d'estimateurs Ridge).\n",
    "si Œª \"grand\" alors Œ≤ÀÜR ‚âà 0.(estimateurs de Ridge seront nulles)\n",
    "Si Œª est positif(augemente) ‚áí le biais augemente et la variance diminue & et r√©ciproquement lorsque Œª negatif(diminue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mod√®le 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a01 = linear_model.Ridge(alpha = 0.1)\n",
    "model_r_a01 = r_a01.fit(X_av, Y_av)\n",
    "Y_hat_av_r01 = r_a01.predict(X_av)\n",
    "Y_hat_t_r01 = r_a01.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r01))\n",
    "print(\"Coefficient de d√©termination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r01))\n",
    "\n",
    "coef = pd.Series(model_r_a01.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mod√®le 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a1 = linear_model.Ridge(alpha = 1)\n",
    "model_r_a1 = r_a1.fit(X_av, Y_av)\n",
    "Y_hat_av_r1 = r_a1.predict(X_av)\n",
    "Y_hat_t_r1 = r_a1.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r1))\n",
    "print(\"Coefficient de d√©termination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r1))\n",
    "\n",
    "coef = pd.Series(model_r_a1.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mod√®le 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a10 = linear_model.Ridge(alpha = 10)\n",
    "model_r_a10 = r_a10.fit(X_av, Y_av)\n",
    "Y_hat_av_r10 = r_a10.predict(X_av)\n",
    "Y_hat_t_r10 = r_a10.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r10))\n",
    "print(\"Coefficient de d√©termination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r10))\n",
    "\n",
    "coef = pd.Series(model_r_a10.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**. Commentez l'√©volution du comportement sur les coefficients. Comment l'expliquez vous ?\n",
    "\n",
    "**Exercice**. Visualisez globalement l'√©volution des coefficients en fonction de alpha. Vous pouvez vous aider de la [documentation sur chemin de r√©gularisation](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html#sphx-glr-auto-examples-linear-model-plot-ridge-path-py).\n",
    "\n",
    "**Exercice**. Mettez en place une proc√©dure d'apprentissage rigoureuse pour trouver le param√®tre alpha optimal de la r√©gression ridge. \n",
    "\n",
    "**Remarque**. Pensez √† regarder les r√©f√©rences fournies en cours (cf. mail et annonce sur le moodle) ainsi que la [documentation sur le mod√®le Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Question Evolution des comportements sur les coefficients</h6><br>\n",
    "A partir des trois modeles, on peut voir qu'ils sont presque √©quivauts sur MSE et R carre. Concernant, les comportement des coeffecients, on peut voir sur le premier modele ou alpha = 0.1 (qui presque identique avec la regression normale qu'on a √©ffectu√© en section 4.1 sur (X_av,Y_av) comme la defintion le dis si alpha pas loin de zero alors les estimateurs de Ridge sont egals a ceux de MCO), ou on vois que le coeffecient le plus impactant est SRMH2O et que le moins impactant est LNO2; <br>\n",
    "puis sur le deuxieme moedele ou alpha = 1, ou on vois que le coeffecient le plus impactant est LNO avec SRMH2O qui passe en deuxieme position et que le moins impactant est LNO2 comme le 1er modele;<br>\n",
    "puis sur le troisieme moedele ou alpha = 10, ou on vois que le coeffecient le plus impactant est STATION_Pla avec LNO qui passe en deuxieme position et que le moins impactant est STATION_Aix et on vois que LNO2 monte d'un cran pour etre avant dernier;\n",
    "\n",
    "<br>On peut expliquer √ß√† avec ce changement de comportements des coeffecients avec la valeur de alpha, car l'estimateur Ridge est dependant du parametre alpha, par exemple si alpha tend vers 0 l'estimateur Ridge equivaut l'estimateur de MCO, et si alpha est trop grand alors estimateur de Ridge equivaut √† zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice. Visualisez globalement l'√©volution des coefficients en fonction de alpha\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# #############################################################################\n",
    "# Compute paths\n",
    "\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(5, 0, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(X_t, Y_t)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "# #############################################################################\n",
    "# Display results\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "coefs = coefs.reshape(200,13)\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  #  reversed axis\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Poids\")\n",
    "plt.title(\"Ridge coefficients as a function of the regularization\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice. Mettez en place une proc√©dure d'apprentissage rigoureuse pour trouver le param√®tre alpha optimal de la r√©gression ridge. \n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeCV = RidgeCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.5, 1,10]).fit(X_av, Y_av)\n",
    "\n",
    "print(\"Le score 'R carre' de la cross validation de Ridge Linear Regression model\", ridgeCV.score(X_av, Y_av))\n",
    "print(\"Les coeffecients du model RidgeCV Linear Regression sont:\",ridgeCV.coef_)\n",
    "print()\n",
    "#print(ridgeCV.intercept_)\n",
    "print(\"Le parametre alpha optimal pour la regression ridge est alpha = \",ridgeCV.alpha_)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "ridge_cv=RidgeCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.5, 1,10], store_cv_values=True)\n",
    "ridge_mod = ridge_cv.fit(X_av,Y_av)\n",
    "ypred = ridge_mod.predict(X_av)\n",
    "\n",
    "x_ax = range(len(X_av))\n",
    "plt.title(\"Comparaison des resultats Y_hat_av avec Y avec alpha = 0.01\") \n",
    "plt.scatter(x_ax,Y_av , s=10, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. R√©gression lin√©aire p√©nalis√©e : Lasso\n",
    "\n",
    "**Question**. Rappelez le mod√®le de la r√©gression lasso et le comportement attendu.\n",
    "\n",
    "**Exercice**. Mettez en place une proc√©dure d'apprentissage rigoureuse pour trouver le param√®tre alpha optimal dans la r√©gression lasso. Pensez √† regarder la [documentation sur le mod√®le Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Question</h6>Lasso Regression model<br>\n",
    "La r√©gression lasso consiste √† minimiser le crit√®re des moindres carr√©s p√©nalis√© par la norme 1 des coefficients.<br>\n",
    "Parmis les comportements attendu du Lasso on a:\n",
    "<br> Si Œª est positif(augemente) ‚áí le biais augemente et la variance diminue & et r√©ciproquement lorsque Œª negatif(diminue).\n",
    "contrairement √† ridge : Œª est grand(augmente) ‚áí le nombre de coefficients nuls augmente,et c'est le cas si deux variables sont corr√©l√©es. L'une sera s√©lectionn√©e par le Lasso, l'autre supprim√©e. C'est aussi son avantage par rapport √† une r√©gression ridge qui ne fera pas de s√©lection de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice. Mettez en place une proc√©dure d'apprentissage rigoureuse pour trouver le param√®tre alpha optimal dans la r√©gression lasso.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lassocv = LassoCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.2,0.5, 1,10,100]).fit(X_av, Y_av)\n",
    "print(\"Le score 'R carre' de la cross validation de Lasso Linear Regression model\", lassocv.score(X_av, Y_av))\n",
    "print(\"Les coeffecients du model LassoCV Linear Regression sont:\",lassocv.coef_)\n",
    "print()\n",
    "print(\"Le parametre alpha optimal pour la regression Lasso est alpha = \",lassocv.alpha_)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "lasso_cv=LassoCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.2,0.5, 1,10,100])\n",
    "lasso_mod = lasso_cv.fit(X_av,Y_av)\n",
    "ypred = lasso_mod.predict(X_av)\n",
    "\n",
    "x_ax = range(len(X_av))\n",
    "plt.title(\"Comparaison des resultats Y_hat_av avec Y avec alpha = 0.1\") \n",
    "plt.scatter(x_ax,Y_av , s=10, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Travail √† rendre\n",
    "\n",
    "Vous devez rendre sur le moodle, pour le dimanche 12/12/21, une archive nom_prenom.(g)zip ou nom_prenom.tgz inf√©rieure √† 10 Mo, contenant :\n",
    "\n",
    "5.1. Un notebook python compl√©tant les r√©ponses aux questions et les exercices de ce nootebook.\n",
    "\n",
    "5.2. Une √©tude sur le jeu de donn√©es [Parkinson speech](https://archive.ics.uci.edu/ml/datasets/Parkinson+Speech+Dataset+with++Multiple+Types+of+Sound+Recordings), en utilisant $Y$ = UPDRS, avec :\n",
    "- Une synth√®se de 4 pages au format pdf contenant les pricipales informations.\n",
    "- Le notebook associ√©."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
