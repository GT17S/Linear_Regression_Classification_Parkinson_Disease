{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP de régression linéaire\n",
    "\n",
    "**Crédits.** Ce TP est largement inspiré du scénario disponible sur Wikistat.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### Objectif\n",
    "\n",
    "L'objectif est d'améliorer la prévision déterministe (MOCAGE), calculée par les services de Météo France, de la concentration d'ozone dans certaines stations de prélèvement. Il s'agit d'un problème dit d'adaptation statistique ou post-traitement d'une prévision locale de modèles à trop grande échelle en s'aidant d'autre variables également gérées par MétéoFrance, mais à plus petite échelle (température, force du vent, etc.).\n",
    "\n",
    "### Description des données \n",
    "Les données ont été extraites et mises en forme par le service concerné de Météo France. Elles sont décrites par les variables suivantes:\n",
    "- JOUR : Type de jour (férié (1) ou pas (0) ;\n",
    "- O3obs : Concentration d'ozone effectivement observée le lendemain à 17h locales correspondant souvent au maximum de pollution observée ;\n",
    "- MOCAGE : Prévision de cette pollution obtenue par un modèle déterministe de mécanique des fluides (équation de Navier et Stockes);\n",
    "- TEMPE : Température prévue par MétéoFrance pour le lendemain 17h ;\n",
    "- RMH2O : Rapport d'humidité ;\n",
    "- NO2 : Concentration en dioxyde d'azote ;\n",
    "- NO : Concentration en monoxyde d'azote ;\n",
    "- STATION : Lieu de l'observation (Aix-en-Provence, Rambouillet, Munchhausen, Cadarache et Plan de Cuques) ;\n",
    "- VentMOD : Force du vent ;\n",
    "- VentANG : Orientation du vent.\n",
    "\n",
    "Ce sont des données bien codées et de petite taille. Elles présentent avant tout un caractère pédagogique.\n",
    "\n",
    "\n",
    "## 2. Prise en main des données\n",
    "\n",
    "Afin de charger et d'étudier les données, nous allons utiliser la librairie pandas pour bénéficier de la classe DataFrame.\n",
    "\n",
    "\n",
    "### 2.1. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt, log\n",
    "\n",
    "# chargement des données\n",
    "Ozone_DF = pd.read_csv('depSeuil.csv', sep = ',', header = 0)\n",
    "Ozone_DF.head()\n",
    "\n",
    "\n",
    "# typage des données\n",
    "Ozone_DF[\"STATION\"] = pd.Categorical(Ozone_DF[\"STATION\"], ordered = False)\n",
    "Ozone_DF[\"JOUR\"] = pd.Categorical(Ozone_DF[\"JOUR\"], ordered = False)\n",
    "Ozone_DF[\"O3obs\"] = pd.DataFrame(Ozone_DF[\"O3obs\"], dtype = float)\n",
    "\n",
    "Ozone_DF.dtypes\n",
    "Ozone_DF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ozone_DF.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploration des données\n",
    "\n",
    "#### Exploration uni-dimensionelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 3)\n",
    "\n",
    "i_name = 0\n",
    "            \n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "            i_name = i_name+1\n",
    "            name_col = Ozone_DF.columns[i_name]\n",
    "            axs[i, j].hist(Ozone_DF[name_col])\n",
    "            axs[i, j].set_title(name_col)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log\n",
    "\n",
    "Ozone_DF[\"SRMH2O\"] = Ozone_DF[\"RMH2O\"].map(lambda x: sqrt(x))\n",
    "Ozone_DF[\"LNO2\"] = Ozone_DF[\"NO2\"].map(lambda x: log(x))\n",
    "Ozone_DF[\"LNO\"] = Ozone_DF[\"NO\"].map(lambda x: log(x))\n",
    "del Ozone_DF[\"RMH2O\"]\n",
    "del Ozone_DF[\"NO2\"]\n",
    "del Ozone_DF[\"NO\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ozone_DF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H6>quelles sont les transformations appliquées ci-dessus, et dans quel but</H6><br>\n",
    "On fait appel à deux fonctions de la librairie Math et on les appliques sur certains colonnes de données de DataFrame Ozone_DF comme suit:<br>\n",
    "1- On fait une racine caree sur toutes les valeurs de la colonne \"RMH2O\" puis par la suite on affecte le resultat dans une autre nouvelle colonne crée qui s'appelle \"SRMH2O\".  <br>\n",
    "2- On fait un logarithme (default base= 'e') sur toutes les valeurs de la colonne \"N2O\" puis par la suite on affecte le resultat dans une autre nouvelle colonne crée qui s'appelle \"LN2O\".<br>\n",
    "3- On fait un logarithme (default base= 'e') sur toutes les valeurs de la colonne \"NO\" puis par la suite on affecte le resultat dans une autre nouvelle colonne crée qui s'appelle \"LNO\".<br>\n",
    "4- On supprime les anciennes colonnes \"RMH2O\", \"N2O\" et \"NO\" du dataframe Ozone_DF.<br>\n",
    "<br>En cas de problèmes, le remède consiste souvent à rechercher des transformations des variables permettant de rendre les distributions symétriques, de “banaliser” les points atypiques et de rendre linéaire la relation. Et c'est le but de ces deux transformations est celui de transformer la relation en relation linéaire. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Quelles sont les transformations appliquées ci-dessus, et dans quel but ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ozone_Dum = pd.get_dummies(Ozone_DF[[\"JOUR\", \"STATION\"]])\n",
    "del Ozone_Dum[\"JOUR_0\"]\n",
    "\n",
    "Ozone_Quant = Ozone_DF[[\"MOCAGE\", \"TEMPE\", \"VentMOD\", \"VentANG\", \"SRMH2O\", \"LNO2\", \"LNO\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ozone_Dum.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ozone_Quant.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H6>quelles sont les transformations appliquées ci-dessus, et dans quel but (2)</H6><br>\n",
    "On fait appel une fonction du la librairie pandas qui est \"get_dummies\" on l'applique sur certains colonnes de données de DataFrame Ozone_DF comme suit:<br>\n",
    "1- On fait appel a get_dummies pour convertir les variables catégorielles en variables fictives(indicateurs), et notamment sur la colonne \"STATION\" de Ozone_DF et aussi la colonne \"JOUR\" de Ozone_DF, par la suite on les stocke dans un Dataframe Ozone_Dum. <br>\n",
    "2- Par la suite on supprime la colonne JOUR_0 du dataframe Ozone_Dum. <br>\n",
    "3- Par la suite on cree un dataframe qui Ozone_Quan qui vas contenir tous les variables/colonnes quantitatives du Ozone_DF (\"MOCAGE\", \"TEMPE\", \"VentMOD\", \"VentANG\", \"SRMH2O\", \"LNO2\", \"LNO\").<br>\n",
    "<br>Le but d'application de cet ensemble de transformations est de séparer les données categorielles (qu'on vas faire des etudes uniquement sur les jours feriers car on a supprimer la colonne JOUR_0 qui represente des jours normales) des données quantitatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Quelles sont les transformations appliquées ci-dessus, et dans quel but ?\n",
    "\n",
    "#### Exploration multi-dimensionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(Ozone_DF[[\"O3obs\", \"MOCAGE\", \"TEMPE\", \"VentMOD\", \"VentANG\", \"SRMH2O\", \"LNO2\", \"LNO\"]], alpha=0.2, figsize=(15, 15), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Création de l'ensemble d'apprentissage / validation et de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([Ozone_Dum, Ozone_Quant], axis = 1)\n",
    "Y = Ozone_DF[\"O3obs\"]\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "X_1, X_2, Y_1, Y_2 = train_test_split(X, Y, train_size = 400, test_size = 200, random_state = 42)\n",
    "\n",
    "X_av = X_1.to_numpy()\n",
    "X_t = X_2.to_numpy()\n",
    "Y_av = np.transpose([Y_1.to_numpy()])\n",
    "Y_t = np.transpose([Y_2.to_numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque.** Scikit-learn fonctionne avec des numpy array. Nous allons donc utiliser les indices des colonnes:\n",
    "\n",
    "0. JOUR\n",
    "1. STATION_Aix \n",
    "2. STATION_Als \n",
    "3. STATION_Cad \n",
    "4. STATION_Pla\n",
    "5. STATION_Ram\n",
    "6. MOCAGE\n",
    "7. TEMPE\n",
    "8. VentMOD\n",
    "9. VentANG\n",
    "10. SRMH2O\n",
    "11. LNO2\n",
    "12. LNO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Régression linéaire univariée\n",
    "\n",
    "### 3.1. Relation unidimensionelle \n",
    "\n",
    "On étudie la qualité de la relation $\\hat{Y} = X$, où $\\hat{Y}$ est le vecteur de prédiction de l'O3 observée et $X$ le vecteur des prévisions de l'O3 observée calculée par Météo France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2)\n",
    "\n",
    "axs[0].plot(X_av[:,[6]], Y_av, 'o')\n",
    "axs[0].plot([0, 300], [0, 300], 'r')\n",
    "axs[0].set_xlabel('Mocage: $X^6 = \\hat{Y}_{av}$')\n",
    "axs[0].set_ylabel('O3 observée: $Y_{av}$')\n",
    "\n",
    "axs[1].plot(X_av[:,[6]], Y_av - X_av[:,[6]], 'o')\n",
    "axs[1].plot([0, 300], [0, 0], 'r')\n",
    "axs[1].set_xlabel('Mocage: $X^6= \\hat{Y}_{av}$')\n",
    "axs[1].set_ylabel('Résidus')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Coefficient de détermination :\", r2_score(Y_av, X_av[:,[6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Commentaires:</h6><br>\n",
    "On peut dire que le coeffecient de determination R carre est tres petit avec une valeur de 0.175 approximativement, ce qui rend ce modele de Y_hat_av = X^6 (mocage) pas de tout efficace, et on vois passer aussi la droite de regression par l'origine deplan mais par sur toutes les valeurs de Y_av ou elle passe pas sur, et avec un niveau de residus tres élevé dans le deuxieme graphe(trop de valeurs \"Y_hat_av - x_mocage\" sont loin d'etre egale à 0, avec une difference qui arrice jusqu'à 150, ce qui fait l'augmentation des residus et aussi la baisse de R carré)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Commentez les résultats.\n",
    "\n",
    "### 3.2. Régression linéaire ordinaire \n",
    "\n",
    "On étudie la qualité de la relation $\\hat{Y} = X\\beta + \\epsilon$, où $\\hat{Y}$ est le vecteur de prédiction de l'O3 observée et $X$ le vecteur des prévisions de l'O3 observée calculée par Météo France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X_av[:,[6]], Y_av)\n",
    "Y_hat_av = ols.predict(X_av[:,[6]])\n",
    "\n",
    "print(\"Coefficient de régression : \\n\", ols.coef_)\n",
    "print()\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2, sharex = True)\n",
    "\n",
    "axs[0].plot(X_av[:,[6]], Y_av, 'o')\n",
    "axs[0].plot(X_av[:,[6]], Y_hat_av, 'r-')\n",
    "#axs[0].set_ylim(0, 300)\n",
    "axs[0].set_xlabel('Régression Mocage: $\\hat{Y}_{av}$')\n",
    "axs[0].set_ylabel('O3 observée: $Y_{av}$')\n",
    "\n",
    "axs[1].plot(Y_hat_av, Y_av - Y_hat_av, 'o')\n",
    "axs[1].plot([0, 300], [0, 0], 'r')\n",
    "axs[1].set_xlabel('Prédiction Mocage: $\\hat{Y}_{av}$')\n",
    "axs[1].set_ylabel('Résidus')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av))\n",
    "print(\"Coefficient de détermination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Un seul coeff:</h6><br>\n",
    "1- On a un seul coeffecient parceque lors du Fit de modele ols (modele de regression lineare) on utlise simplement une seule variable indepandente qui est X6_Mocage du dataframe X_av.\n",
    "<br><h6>Commentaires:</h6><br>\n",
    "On peut dire que le coeffecient de determination R carre est petit avec une valeur de 0.35 approximativement, ce qui rend ce modele de Y_hat_av = beta * X^6 (mocage) pas de tout efficace(tel qu'il est le cas pour Y_hat_av=x6_mocage), et on vois passer la droite de regression par certains  points (valeurs <==> que y'a certaines predictions juste) de Y_av mais pas tous, et on a moyenne de la différence au carré des valeurs réelles par rapport aux valeurs prévues (MSE) égale à 1125,92 qui comme meme trop grand. Et avec un niveau de residus tres élevé dans le deuxieme graphe(trop de valeurs residus \"Y_av - Y_hat_av\" sont loin d'etre egale à 0, ce qui fait l'augmentation des residus et aussi la baisse de R carré )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions.**\n",
    "\n",
    "1. Pourquoi n'y a-t-il qu'un seul coefficient ?\n",
    "2. Commentez les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Régression linéaire multivariée\n",
    "\n",
    "### 4.1. Régression linéaire ordinaire\n",
    "\n",
    "**Exercice.** \n",
    "1. Appliquez une régression linéaire sur l'ensemble d'apprentissage $(X_{av}, Y_{av})$ et évaluez la qualité des résultats en comparant ceux obtenus sur $(X_{av}, Y_{av})$ et sur $(X_{t}, Y_{t})$. Commentez.\n",
    "2. Observez les coefficients de régression et commentez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application de la regression lineaire multivalué (𝑋𝑎𝑣,𝑌𝑎𝑣). \n",
    "from sklearn import linear_model\n",
    "\n",
    "#creation de modele de regression lineaire\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "#entrainement de model\n",
    "reg.fit(X_av, Y_av)\n",
    "\n",
    "#affichage des coeffecients de regression pour le model\n",
    "print(\"Coffecients de regression\",reg.coef_)\n",
    "print()\n",
    "\n",
    "#prediction de model sur les deux sets X_av et X_t\n",
    "Y_hat_av41 = reg.predict(X_av)\n",
    "Y_hat_t41 = reg.predict(X_t)\n",
    "\n",
    "print(\"****************Y_av ****************************\")\n",
    "print(\"Erreur quadratique (learning set) Y_av: %.2f\" % mean_squared_error(Y_av, Y_hat_av41))\n",
    "print(\"Coefficient de détermination (learning set) de Y_av: %.2f\" % r2_score(Y_av, Y_hat_av41))\n",
    "print(\"*************************************************\")\n",
    "print()\n",
    "\n",
    "print(\"****************Y_t *****************************\")\n",
    "print(\"Erreur quadratique (testing set) Y_t: %.2f\" % mean_squared_error(Y_t, Y_hat_t41))\n",
    "print(\"Coefficient de détermination (testing set) de Y_t: %.2f\" % r2_score(Y_t, Y_hat_t41))\n",
    "print(\"**************************************************\")\n",
    "\n",
    "print()\n",
    "\n",
    "#affichage des coeffecients de regression pour question 2\n",
    "coef = pd.Series(reg.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients pour regression lineaire multivalué\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Question 1:</h6><br> \n",
    "A partir des resultats obtenu, le coeffecient de determination (R carre) et la moyenne de la différence au carré des valeurs réelles par rapport aux valeurs prévues (MSE), on peut voir clairement que la qualité de couple (𝑋𝑎𝑣,𝑌𝑎𝑣) est mielleur de l'autre couple (𝑋𝑡,𝑌𝑡), ou on peu voir que le R carre de Y_hat_av41 du couple (𝑋av,𝑌av) est superieur à celui de (𝑋𝑡,𝑌𝑡), 0.57 > 0.42 qui implique que l'accuracy de model est mielleur sur le premier couple que au deuxieme. Et aussi on vois que le MSE de (𝑋𝑡,𝑌𝑡) est superieur à celui de MSE (𝑋av,𝑌av), 959.51 > 742.01 qui veut dire que le nombre de résidus est supérieur sur le couple (𝑋𝑡,𝑌𝑡) qu'au couple (𝑋av,𝑌av), ainsi éxpliquant la qualité surpassante de modele de regression sur le couple (𝑋av,𝑌av) à celui de (𝑋𝑡,𝑌𝑡).\n",
    "\n",
    "\n",
    "<br><h6>Question 2:</h6><br>\n",
    "On peut voir sur le plot précedent (Coefficients pour regression lineaire multivalué) que les variables les plus impactantes sur le modeles sont:<br>\n",
    "on a au premier lieu, SRMHO2 est la variable indepandante la plus impactante apres on a LNO et ainsi de suite jusqu'au moins important qui est LNO2.<br>\n",
    "Et on peut aussi voir que ce modele de regression il a pris tout les variables indepandantes, car elles sont toutes representés sur le plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Régression linéaire pénalisée : Ridge\n",
    "\n",
    "**Question**.  Rappelez le modèle de la régression ridge et le comportement attendu.\n",
    "\n",
    "Nous allons observer sur l'ensemble d'apprentissage, pour 3 valeurs de alpha ($\\lambda$ dans le cours), le comportement de la regression ridge sur les coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Question</h6>RIDGE Regression model<br>\n",
    "La régression ridge consiste à minimiser le critère des moindres carrés pénalisé par la norme 2 des coefficients.\n",
    "<br>\n",
    "Parmis les comportements attendu du Ridge on a:<br>\n",
    "Si λ (alpha de Ridge) = 0, on retrouve le biais et la variance de l’estimateur des Moindres Carres Ordinaire (y'aura pas d'estimateurs Ridge).\n",
    "si λ \"grand\" alors βˆR ≈ 0.(estimateurs de Ridge seront nulles)\n",
    "Si λ est positif(augemente) ⇒ le biais augemente et la variance diminue & et réciproquement lorsque λ negatif(diminue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a01 = linear_model.Ridge(alpha = 0.1)\n",
    "model_r_a01 = r_a01.fit(X_av, Y_av)\n",
    "Y_hat_av_r01 = r_a01.predict(X_av)\n",
    "Y_hat_t_r01 = r_a01.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r01))\n",
    "print(\"Coefficient de détermination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r01))\n",
    "\n",
    "coef = pd.Series(model_r_a01.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a1 = linear_model.Ridge(alpha = 1)\n",
    "model_r_a1 = r_a1.fit(X_av, Y_av)\n",
    "Y_hat_av_r1 = r_a1.predict(X_av)\n",
    "Y_hat_t_r1 = r_a1.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r1))\n",
    "print(\"Coefficient de détermination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r1))\n",
    "\n",
    "coef = pd.Series(model_r_a1.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_a10 = linear_model.Ridge(alpha = 10)\n",
    "model_r_a10 = r_a10.fit(X_av, Y_av)\n",
    "Y_hat_av_r10 = r_a10.predict(X_av)\n",
    "Y_hat_t_r10 = r_a10.predict(X_t)\n",
    "\n",
    "print(\"Erreur quadratique (learning set) : %.2f\" % mean_squared_error(Y_av, Y_hat_av_r10))\n",
    "print(\"Coefficient de détermination (learning set) : %.2f\" % r2_score(Y_av, Y_hat_av_r10))\n",
    "\n",
    "coef = pd.Series(model_r_a10.coef_[0], index =  X.columns)\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients ridge pour alpha = 10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**. Commentez l'évolution du comportement sur les coefficients. Comment l'expliquez vous ?\n",
    "\n",
    "**Exercice**. Visualisez globalement l'évolution des coefficients en fonction de alpha. Vous pouvez vous aider de la [documentation sur chemin de régularisation](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html#sphx-glr-auto-examples-linear-model-plot-ridge-path-py).\n",
    "\n",
    "**Exercice**. Mettez en place une procédure d'apprentissage rigoureuse pour trouver le paramètre alpha optimal de la régression ridge. \n",
    "\n",
    "**Remarque**. Pensez à regarder les références fournies en cours (cf. mail et annonce sur le moodle) ainsi que la [documentation sur le modèle Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Question Evolution des comportements sur les coefficients</h6><br>\n",
    "A partir des trois modeles, on peut voir qu'ils sont presque équivauts sur MSE et R carre. Concernant, les comportement des coeffecients, on peut voir sur le premier modele ou alpha = 0.1 (qui presque identique avec la regression normale qu'on a éffectué en section 4.1 sur (X_av,Y_av) comme la defintion le dis si alpha pas loin de zero alors les estimateurs de Ridge sont egals a ceux de MCO), ou on vois que le coeffecient le plus impactant est SRMH2O et que le moins impactant est LNO2; <br>\n",
    "puis sur le deuxieme moedele ou alpha = 1, ou on vois que le coeffecient le plus impactant est LNO avec SRMH2O qui passe en deuxieme position et que le moins impactant est LNO2 comme le 1er modele;<br>\n",
    "puis sur le troisieme moedele ou alpha = 10, ou on vois que le coeffecient le plus impactant est STATION_Pla avec LNO qui passe en deuxieme position et que le moins impactant est STATION_Aix et on vois que LNO2 monte d'un cran pour etre avant dernier;\n",
    "\n",
    "<br>On peut expliquer çà avec ce changement de comportements des coeffecients avec la valeur de alpha, car l'estimateur Ridge est dependant du parametre alpha, par exemple si alpha tend vers 0 l'estimateur Ridge equivaut l'estimateur de MCO, et si alpha est trop grand alors estimateur de Ridge equivaut à zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice. Visualisez globalement l'évolution des coefficients en fonction de alpha\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# #############################################################################\n",
    "# Compute paths\n",
    "\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(5, 0, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(X_t, Y_t)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "# #############################################################################\n",
    "# Display results\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "coefs = coefs.reshape(200,13)\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  #  reversed axis\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Poids\")\n",
    "plt.title(\"Ridge coefficients as a function of the regularization\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice. Mettez en place une procédure d'apprentissage rigoureuse pour trouver le paramètre alpha optimal de la régression ridge. \n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeCV = RidgeCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.5, 1,10]).fit(X_av, Y_av)\n",
    "\n",
    "print(\"Le score 'R carre' de la cross validation de Ridge Linear Regression model\", ridgeCV.score(X_av, Y_av))\n",
    "print(\"Les coeffecients du model RidgeCV Linear Regression sont:\",ridgeCV.coef_)\n",
    "print()\n",
    "#print(ridgeCV.intercept_)\n",
    "print(\"Le parametre alpha optimal pour la regression ridge est alpha = \",ridgeCV.alpha_)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "ridge_cv=RidgeCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.5, 1,10], store_cv_values=True)\n",
    "ridge_mod = ridge_cv.fit(X_av,Y_av)\n",
    "ypred = ridge_mod.predict(X_av)\n",
    "\n",
    "x_ax = range(len(X_av))\n",
    "plt.title(\"Comparaison des resultats Y_hat_av avec Y avec alpha = 0.01\") \n",
    "plt.scatter(x_ax,Y_av , s=10, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Régression linéaire pénalisée : Lasso\n",
    "\n",
    "**Question**. Rappelez le modèle de la régression lasso et le comportement attendu.\n",
    "\n",
    "**Exercice**. Mettez en place une procédure d'apprentissage rigoureuse pour trouver le paramètre alpha optimal dans la régression lasso. Pensez à regarder la [documentation sur le modèle Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Question</h6>Lasso Regression model<br>\n",
    "La régression lasso consiste à minimiser le critère des moindres carrés pénalisé par la norme 1 des coefficients.<br>\n",
    "Parmis les comportements attendu du Lasso on a:\n",
    "<br> Si λ est positif(augemente) ⇒ le biais augemente et la variance diminue & et réciproquement lorsque λ negatif(diminue).\n",
    "contrairement à ridge : λ est grand(augmente) ⇒ le nombre de coefficients nuls augmente,et c'est le cas si deux variables sont corrélées. L'une sera sélectionnée par le Lasso, l'autre supprimée. C'est aussi son avantage par rapport à une régression ridge qui ne fera pas de sélection de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice. Mettez en place une procédure d'apprentissage rigoureuse pour trouver le paramètre alpha optimal dans la régression lasso.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lassocv = LassoCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.2,0.5, 1,10,100]).fit(X_av, Y_av)\n",
    "print(\"Le score 'R carre' de la cross validation de Lasso Linear Regression model\", lassocv.score(X_av, Y_av))\n",
    "print(\"Les coeffecients du model LassoCV Linear Regression sont:\",lassocv.coef_)\n",
    "print()\n",
    "print(\"Le parametre alpha optimal pour la regression Lasso est alpha = \",lassocv.alpha_)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "lasso_cv=LassoCV(alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,0.2,0.5, 1,10,100])\n",
    "lasso_mod = lasso_cv.fit(X_av,Y_av)\n",
    "ypred = lasso_mod.predict(X_av)\n",
    "\n",
    "x_ax = range(len(X_av))\n",
    "plt.title(\"Comparaison des resultats Y_hat_av avec Y avec alpha = 0.1\") \n",
    "plt.scatter(x_ax,Y_av , s=10, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Travail à rendre\n",
    "\n",
    "Vous devez rendre sur le moodle, pour le dimanche 12/12/21, une archive nom_prenom.(g)zip ou nom_prenom.tgz inférieure à 10 Mo, contenant :\n",
    "\n",
    "5.1. Un notebook python complétant les réponses aux questions et les exercices de ce nootebook.\n",
    "\n",
    "5.2. Une étude sur le jeu de données [Parkinson speech](https://archive.ics.uci.edu/ml/datasets/Parkinson+Speech+Dataset+with++Multiple+Types+of+Sound+Recordings), en utilisant $Y$ = UPDRS, avec :\n",
    "- Une synthèse de 4 pages au format pdf contenant les pricipales informations.\n",
    "- Le notebook associé."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
